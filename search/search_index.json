{"config":{"lang":["en"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"\ud83d\udc36 doggAI \u2014 \u6253\u9020\u4f60\u81ea\u5df1\u7684 AI \u597d\u5925\u4f34","text":"<p>\u7a69\u5065\u53ef\u9760\u3001\u53ef\u611b\u6eab\u99a8\uff0c\u9019\u88e1\u662f doggAI \u7684\u5bb6\u3002 \u4e00\u500b 30 \u5929\u5be6\u4f5c\u4e4b\u65c5\uff0c\u5f9e\u5730\u7aef\u5230\u667a\u6167\uff0c\u5f9e\u7a0b\u5f0f\u5230\u60c5\u611f\u3002</p>"},{"location":"#_1","title":"\u2728 \u8a08\u756b\u76ee\u6a19","text":"<ul> <li>\u81ea\u67b6 LLM \u5e73\u53f0\u5b8c\u6574\u6d41\u7a0b</li> <li>\u5f9e\u786c\u9ad4\u9078\u578b\u3001\u67b6\u69cb\u90e8\u7f72\u3001\u6a21\u578b\u7ba1\u7406\u3001\u518d\u5230\u6e2c\u8a66\u8207\u512a\u5316</li> <li>\u7528\u611b\u5beb\u7a0b\u5f0f\uff0c\u7528\u5fc3\u90e8\u7f72 AI</li> </ul>"},{"location":"#_2","title":"\ud83d\udcd6 \u5be6\u4f5c\u65e5\u8a8c\u7d22\u5f15","text":"<p>\u8acb\u9ede\u9078\u5de6\u5074\u300c\u6bcf\u65e5\u5be6\u4f5c\u65e5\u8a8c\u300d\u67e5\u770b\u6bcf\u65e5\u5167\u5bb9\u3002</p>"},{"location":"day1/","title":"\ud83d\udcc5 Day 1\uff1a\u555f\u7a0b\uff01doggAI \u7684\u8a95\u751f","text":"<p>\u4eca\u5929\uff0c\u6211\u5011\u958b\u59cb\u4e86 doggAI \u7684 30 \u5929\u65c5\u7a0b\u3002</p>"},{"location":"day1/#_1","title":"\u2705 \u76ee\u6a19","text":"<ul> <li>\u78ba\u7acb\u8a08\u756b\u4e3b\u984c\u8207\u67b6\u69cb</li> <li>\u5efa\u7acb MkDocs \u5c08\u6848</li> <li>\u767c\u4f48\u7b2c\u4e00\u7bc7\u5167\u5bb9\u5230 GitHub Pages</li> </ul>"},{"location":"day1/#_2","title":"\ud83d\udee0 \u5be6\u4f5c","text":"<pre><code>pip install mkdocs mkdocs-material\nmkdocs new doggAI\ncd doggAI\nmkdocs serve\n</code></pre> <p>\u4e0b\u4e00\u6b65\uff0c\u6211\u5011\u5c07\u958b\u59cb\u63a2\u8a0e\u786c\u9ad4\u74b0\u5883\u5efa\u7f6e\u8207\u8cc7\u6e90\u914d\u7f6e\u7b56\u7565\u3002</p>"},{"location":"logs/day-template/","title":"\ud83d\udcc5 Day X\uff1a\u6a19\u984c","text":""},{"location":"logs/day-template/#_1","title":"\ud83c\udfaf \u4eca\u65e5\u76ee\u6a19","text":"<ul> <li>\u4f60\u4eca\u5929\u60f3\u5b8c\u6210\u4ec0\u9ebc\uff1f</li> </ul>"},{"location":"logs/day-template/#_2","title":"\ud83d\udee0\ufe0f \u4eca\u65e5\u7d00\u9304","text":"<ul> <li>\u4f60\u505a\u4e86\u54ea\u4e9b\u4e8b\uff1f</li> <li>\u767c\u751f\u4ec0\u9ebc\u6709\u8da3\u7684\u554f\u984c\uff1f</li> </ul>"},{"location":"logs/day-template/#_3","title":"\ud83d\udd2e \u660e\u65e5\u9810\u544a","text":"<p>\u660e\u5929\u60f3\u505a\u4ec0\u9ebc\uff1f</p>"},{"location":"logs/day1/","title":"Day1","text":"<p>\u5efa\u7acb\u672c\u5730\u7aef LLM \u74b0\u5883\uff0c\u6e2c\u8a66 API \u529f\u80fd\uff0c\u4e26\u5efa\u7acb\u5b8c\u6574\u7684\u6548\u80fd\u76e3\u63a7\u7cfb\u7d71\u3002  </p> <p>\u4f60\u7684\u554f\u984c: \u5efa\u7f6e\u672c\u5730\u7aef LLM \u9700\u8981\u4ec0\u9ebc\u786c\u9ad4\u898f\u683c\uff1f \u6211\u7684\u5efa\u8b70: \u63d0\u4f9b\u4e86\u5165\u9580\u7d1a\u3001\u4e2d\u968e\u7d1a\u3001\u9ad8\u968e\u7d1a\u4e09\u7a2e\u914d\u7f6e\u65b9\u6848 \u4f60\u7684\u6c7a\u5b9a: - \u521d\u59cb\u8003\u616e\uff1a\u820a\u684c\u6a5f (i7-8700 + 32GB RAM + Intel UHD 128MB) - \u6700\u7d42\u9078\u64c7: \u7b46\u96fb (i9-13900H + 32GB DDR5 + RTX 4070 8GB) \u2705 - ### 2. \u4f5c\u696d\u7cfb\u7d71\u9078\u64c7\u8a0e\u8ad6</p> <p>\u8a0e\u8ad6\u91cd\u9ede: Ubuntu Server vs Desktop\uff0c22.04 vs 24.04 vs 25.04 \u4f60\u7684\u6c7a\u5b9a: Ubuntu 24.04 LTS Desktop \u7406\u7531: \u9700\u8981\u5716\u5f62\u754c\u9762\u6e2c\u8a66 API\uff0c\u9577\u671f\u652f\u63f4\u5230 2029\u5e74 - ### 3. LLM \u7ba1\u7406\u5de5\u5177\u9078\u64c7</p> <p>\u6211\u63d0\u4f9b\u7684\u9078\u9805: Ollama, LM Studio, Text Gen WebUI, vLLM, llama.cpp \u4f60\u7684\u6c7a\u5b9a: \u4e3b\u8981\u4f7f\u7528 Ollama \u7406\u7531: \u5e73\u8861\u6613\u7528\u6027\u548c\u529f\u80fd\u6027 - ### 4. \u7cfb\u7d71\u67b6\u69cb\u78ba\u8a8d</p> <p>\u767c\u73fe: \u4f60\u5df2\u6709 Windows 11 + WSL2 Ubuntu \u74b0\u5883 \u6c7a\u5b9a: \u4f7f\u7528\u73fe\u6709\u7684 WSL2 \u74b0\u5883\uff0c\u4e0d\u9700\u8981\u91cd\u65b0\u5b89\u88dd  </p> <p><pre><code># \u78ba\u8a8d WSL2 \u7248\u672c\nwsl --version  # \u7d50\u679c\uff1a2.5.9.0 \u2705\n\n# \u78ba\u8a8d CUDA \u74b0\u5883\nnvcc --version  # \u7d50\u679c\uff1aCUDA 12.6 \u2705\nnvidia-smi     # \u7d50\u679c\uff1aRTX 4070 \u6b63\u5e38\u8b58\u5225 \u2705\n</code></pre> - ### Phase 2: \u9047\u5230\u7684\u56f0\u96e3\u8207\u89e3\u6c7a</p> <p>\u554f\u984c 1: \u5fd8\u8a18 sudo \u5bc6\u78bc \u89e3\u6c7a\u65b9\u6848:  </p> <pre><code>wsl -u root\npasswd your_username  # \u91cd\u8a2d\u5bc6\u78bc\u6210\u529f \u2705\n</code></pre> <p>\u554f\u984c 2: NVIDIA Container Toolkit \u5b89\u88dd\u8b66\u544a \u73fe\u8c61: \"apt-key is deprecated\" \u8b66\u544a \u89e3\u6c7a: \u78ba\u8a8d\u9019\u662f\u6b63\u5e38\u8b66\u544a\uff0c\u529f\u80fd\u6b63\u5e38  </p> <p>\u554f\u984c 3: nvtop \u7121\u6cd5\u8b58\u5225 GPU \u89e3\u6c7a: \u6539\u7528\u81ea\u88fd\u7684 <code>gpu_monitor.sh</code> \u8173\u672c  </p> <p>\u554f\u984c 4: Ollama port \u885d\u7a81 \u73fe\u8c61: \"bind: address already in use\" \u89e3\u6c7a: \u767c\u73fe Ollama \u5df2\u5728\u904b\u884c\uff0c\u76f4\u63a5\u4f7f\u7528\u73fe\u6709\u670d\u52d9 - ### Phase 3: \u6a21\u578b\u6e2c\u8a66\u8207\u6548\u80fd\u9a57\u8b49</p> <pre><code># \u5b89\u88dd Ollama (\u6210\u529f)\ncurl -fsSL https://ollama.ai/install.sh | sh\n\n# \u4e0b\u8f09\u6a21\u578b\nollama pull llama2:7b    # \u7d04 4GB\uff0c\u4e0b\u8f09\u6210\u529f\nollama pull qwen2.5:7b   # \u4e2d\u6587\u6a21\u578b\uff0c\u4e0b\u8f09\u6210\u529f\n\n# \u57fa\u672c\u6e2c\u8a66\nollama run llama2:7b \"Hello, tell me about yourself\"  # \u6210\u529f\u904b\u884c\n</code></pre> <pre><code>- ## \ud83d\udcca \u6548\u80fd\u6e2c\u8a66\u7d50\u679c\n- ### \u786c\u9ad4\u8868\u73fe\u7d71\u8a08\n\n  | \u6e2c\u8a66\u9805\u76ee | \u7d50\u679c | GPU\u4f7f\u7528\u7387 | GPU\u8a18\u61b6\u9ad4 | \u6eab\u5ea6 | \u529f\u8017 |\n  |---|---|---|---|---|---|\n  | \u57fa\u672c\u5c0d\u8a71 | 7.07\u79d2 | 95% | 78.3% | 55\u00b0C | 45W |\n  | \u4e32\u6d41\u56de\u61c9 | 13.76\u79d2 | 96% | 78.3% | 55\u00b0C | 41W |\n  | REST API | 4.65\u79d2 | 95% | 78.4% | 55\u00b0C | 43.5W |\n  | \u7c21\u55ae\u554f\u7b54 | 2.63\u79d2 | 96% | 78.4% | 55\u00b0C | 30.4W |\n</code></pre> <ul> <li>\u6eab\u5ea6\u63a7\u5236\u512a\u79c0: \u6700\u9ad8\u50c5 55\u00b0C\uff0c\u6563\u71b1\u9918\u88d5\u5927</li> <li>GPU \u5229\u7528\u7387\u5b8c\u7f8e: 95-96%\uff0c\u5b8c\u5168\u767c\u63ee\u786c\u9ad4\u6548\u80fd</li> <li>\u529f\u8017\u5408\u7406: 30-45W\uff0c\u76f8\u7576\u7bc0\u80fd</li> <li>\u56de\u61c9\u901f\u5ea6\u7406\u60f3: 2-14\u79d2\u7bc4\u570d\uff0c\u7b26\u5408\u9810\u671f</li> </ul> <ul> <li> </li> <li> </li> </ul> <p><pre><code>~/gpu_monitor.sh  # \u5373\u6642\u76e3\u63a7 GPU \u72c0\u614b\n</code></pre> - ### 2. \u7cfb\u7d71\u76e3\u63a7\u8173\u672c</p> <p><pre><code>~/monitor_system.sh  # \u7d9c\u5408\u7cfb\u7d71\u8cc7\u6e90\u76e3\u63a7\n</code></pre> - ### 3. API \u6e2c\u8a66\u8173\u672c - <code>~/test_api.py</code> - \u57fa\u672c API \u6e2c\u8a66 - <code>~/test_streaming.py</code> - \u4e32\u6d41\u56de\u61c9\u6e2c\u8a66 - <code>~/performance_test.py</code> - \u6548\u80fd\u57fa\u6e96\u6e2c\u8a66 - ### 4. \u81ea\u52d5\u5316\u6e2c\u8a66\u6846\u67b6 \u2b50</p> <pre><code>~/auto_test_framework.py  # \u5b8c\u6574\u7684\u81ea\u52d5\u5316\u6e2c\u8a66\u8207\u8a18\u9304\u7cfb\u7d71\n</code></pre> <p>\u529f\u80fd:     - \u81ea\u52d5\u57f7\u884c\u591a\u9805\u6e2c\u8a66     - \u8a18\u9304 GPU/\u7cfb\u7d71\u72c0\u614b     - \u751f\u6210 JSON \u683c\u5f0f\u8a73\u7d30\u8a18\u9304     - \u63d0\u4f9b\u6e2c\u8a66\u6458\u8981\u5831\u544a</p> <pre><code>- ## \ud83c\udfaf \u9054\u6210\u7684\u91cc\u7a0b\u7891\n- ### \u2705 \u74b0\u5883\u5efa\u7f6e\u5b8c\u6210\n- WSL2 + Ubuntu 24.04 LTS \u74b0\u5883\u5c31\u7dd2\n- NVIDIA GPU \u9a45\u52d5 + CUDA 12.6 \u6b63\u5e38\u904b\u4f5c\n- Ollama LLM \u7ba1\u7406\u7cfb\u7d71\u5b89\u88dd\u5b8c\u6210\n- ### \u2705 \u6a21\u578b\u6e2c\u8a66\u6210\u529f\n- Llama2 7B \u6a21\u578b\u6b63\u5e38\u904b\u884c\n- Qwen2.5 7B \u4e2d\u6587\u6a21\u578b\u6b63\u5e38\u904b\u884c\n- API \u529f\u80fd\u6e2c\u8a66\u901a\u904e\n- ### \u2705 \u76e3\u63a7\u7cfb\u7d71\u5efa\u7acb\n- \u5373\u6642 GPU \u76e3\u63a7\n- \u6548\u80fd\u6578\u64da\u81ea\u52d5\u8a18\u9304\n- \u5b8c\u6574\u7684\u6e2c\u8a66\u6846\u67b6\n- ### \u2705 \u6548\u80fd\u9a57\u8b49\u5b8c\u6210\n- RTX 4070 \u6548\u80fd\u5b8c\u5168\u767c\u63ee\n- \u6eab\u5ea6\u63a7\u5236\u5728\u7406\u60f3\u7bc4\u570d\n- \u56de\u61c9\u901f\u5ea6\u7b26\u5408\u9810\u671f\n</code></pre> <pre><code>- ## \ud83d\udd2e \u4e0b\u6b21\u53ef\u4ee5\u7e7c\u7e8c\u7684\u65b9\u5411\n- ### 1. \u6a21\u578b\u63a2\u7d22\n    - \u6e2c\u8a66 13B \u66f4\u5927\u6a21\u578b\n    - \u5617\u8a66\u7a0b\u5f0f\u78bc\u751f\u6210\u6a21\u578b (CodeLlama)\n    - \u6e2c\u8a66\u591a\u6a21\u614b\u6a21\u578b (LLaVA)\n- ### 2. \u61c9\u7528\u958b\u767c\n    - \u5efa\u7acb Web UI \u754c\u9762\n    - \u958b\u767c Python API \u5ba2\u6236\u7aef\n    - \u6574\u5408 FastAPI \u5f8c\u7aef\u670d\u52d9\n- ### 3. \u6548\u80fd\u512a\u5316\n    - \u4e26\u884c\u8655\u7406\u6e2c\u8a66\n    - \u8a18\u61b6\u9ad4\u4f7f\u7528\u512a\u5316\n    - \u91cf\u5316\u6a21\u578b\u6e2c\u8a66\n- ### 4. \u751f\u7522\u74b0\u5883\u6e96\u5099\n    - Docker \u5bb9\u5668\u5316\n    - \u8ca0\u8f09\u6e2c\u8a66\n    - \u76e3\u63a7\u7cfb\u7d71\u5b8c\u5584\n</code></pre>"},{"location":"logs/day1/#20250726-llm","title":"2025/07/26 \u672c\u5730 LLM \u90e8\u7f72\u5b8c\u6574\u8a18\u9304","text":""},{"location":"logs/day1/#_1","title":"\ud83d\udccb \u4eca\u65e5\u76ee\u6a19","text":""},{"location":"logs/day1/#_2","title":"\ud83d\udde3\ufe0f \u8a0e\u8ad6\u6b77\u7a0b\u8207\u6c7a\u7b56","text":""},{"location":"logs/day1/#1","title":"1. \u786c\u9ad4\u914d\u7f6e\u8a0e\u8ad6","text":""},{"location":"logs/day1/#_3","title":"\ud83d\udcbb \u5be6\u969b\u64cd\u4f5c\u8a18\u9304","text":""},{"location":"logs/day1/#phase-1","title":"Phase 1: \u74b0\u5883\u6aa2\u67e5\u8207\u6e96\u5099","text":""},{"location":"logs/day1/#_4","title":"\u95dc\u9375\u767c\u73fe","text":""},{"location":"logs/day1/#_5","title":"\ud83d\udee0\ufe0f \u5efa\u7acb\u7684\u5de5\u5177\u8207\u8173\u672c","text":""},{"location":"logs/day1/#1-gpu","title":"1. GPU \u76e3\u63a7\u8173\u672c","text":""},{"location":"logs/day1/#-","title":"-","text":"<pre><code>- ## \ud83d\udcc8 \u7e3d\u7d50\u8a55\u50f9\n\n  **\u4eca\u65e5\u6210\u5c31\u7b49\u7d1a**: \ud83c\udfc6 **\u5c08\u696d\u7d1a**\n\n  **\u6548\u80fd\u8a55\u5206**:  \n    - GPU \u5229\u7528\u7387: A+ (95-96%)\n    - \u6563\u71b1\u63a7\u5236: A+ (55\u00b0C)\n    - \u56de\u61c9\u901f\u5ea6: A (2-14\u79d2)\n    - \u529f\u8017\u6548\u7387: A+ (30-45W)\n    - \u7cfb\u7d71\u7a69\u5b9a\u6027: A+ (\u7121\u7576\u6a5f\u6216\u932f\u8aa4)\n\n      **\u4f60\u7684 RTX 4070 + WSL2 \u7d44\u5408\u8868\u73fe\u8d85\u51fa\u9810\u671f\uff0c\u5b8c\u5168\u5177\u5099\u4e86\u904b\u884c\u672c\u5730 LLM \u7684\u5c08\u696d\u7d1a\u80fd\u529b\uff01** \ud83d\ude80\n</code></pre> <pre><code>      *\u8a18\u9304\u6642\u9593: 2025/07/26 \u5b8c\u6210*  \n      *\u4e0b\u6b21\u7e7c\u7e8c: \u6839\u64da\u9700\u6c42\u9078\u64c7\u4e0a\u8ff0\u4efb\u4e00\u65b9\u5411\u6df1\u5165\u63a2\u7d22*\n\n      &lt;!--EndFragment--&gt;\n</code></pre>"},{"location":"logs/day2/","title":"Day2","text":"<p><pre><code>#!/bin/bash\n\nwhile true; do\n  clear\n  echo \"=== GPU \u76e3\u63a7 (\u6309 Ctrl+C \u505c\u6b62) ===\"\n  echo \"\u6642\u9593: $(date)\"\n  echo \"\"\n\n  # GPU \u57fa\u672c\u8cc7\u8a0a\n  nvidia-smi --query-gpu=name,memory.used,memory.total,utilization.gpu,temperature.gpu,power.draw --format=csv,noheader,nounits | \\\n  awk -F', ' '{\n      printf \"GPU: %s\\n\", $1\n      printf \"\u8a18\u61b6\u9ad4: %s MB / %s MB (%.1f%%)\\n\", $2, $3, ($2/$3)*100\n      printf \"\u4f7f\u7528\u7387: %s%%\\n\", $4\n      printf \"\u6eab\u5ea6: %s\u00b0C\\n\", $5\n      printf \"\u529f\u8017: %s W\\n\", $6\n  }'\n\n  echo \"\"\n  echo \"\u57f7\u884c\u4e2d\u7684 GPU \u7a0b\u5e8f:\"\n  nvidia-smi --query-compute-apps=pid,process_name,used_memory --format=csv,noheader,nounits 2&gt;/dev/null || echo \"\u7121 GPU \u7a0b\u5e8f\u904b\u884c\"\n\n  sleep 2\ndone\n</code></pre> - ## A2. \u7cfb\u7d71\u76e3\u63a7\u8173\u672c - ### <code>monitor_system.sh</code></p> <p><pre><code>#!/bin/bash\n\necho \"=== \u7cfb\u7d71\u8cc7\u6e90\u76e3\u63a7 ===\"\necho \"\u6642\u9593: $(date)\"\necho \"\"\n\necho \"1. \u8a18\u61b6\u9ad4\u4f7f\u7528\u72c0\u6cc1:\"\nfree -h\necho \"\"\n\necho \"2. \u78c1\u789f\u4f7f\u7528\u72c0\u6cc1:\"\ndf -h | grep -E \"(\u6839\u76ee\u9304|/dev/)\"\necho \"\"\n\necho \"3. CPU \u8ca0\u8f09:\"\nuptime\necho \"\"\n\necho \"4. GPU \u72c0\u6cc1:\"\nnvidia-smi --query-gpu=name,memory.used,memory.total,utilization.gpu,temperature.gpu,power.draw --format=csv,noheader,nounits\necho \"\"\n\necho \"5. \u524d 10 \u500b\u6d88\u8017\u8a18\u61b6\u9ad4\u7684\u7a0b\u5e8f:\"\nps aux --sort=-%mem | head -11\necho \"\"\n</code></pre> - ## A3. API \u6e2c\u8a66\u8173\u672c - ### <code>test_api.py</code></p> <p><pre><code>import ollama\nimport time\n\ndef test_performance():\n  print(\"\u958b\u59cb\u6e2c\u8a66 Ollama API...\")\n  start_time = time.time()\n\n  response = ollama.chat(model='llama2:7b', messages=[\n      {'role': 'user', 'content': 'Count from 1 to 5 and briefly explain each number'}\n  ])\n\n  end_time = time.time()\n\n  print(f\"\u56de\u61c9\u6642\u9593: {end_time - start_time:.2f} \u79d2\")\n  print(\"=\"*50)\n  print(\"\u5b8c\u6574\u56de\u61c9\u5167\u5bb9:\")\n  print(response['message']['content'])\n  print(\"=\"*50)\n\nif __name__ == \"__main__\":\n  test_performance()\n</code></pre> - ### <code>test_streaming.py</code></p> <p><pre><code>import ollama\nimport time\n\ndef test_streaming():\n  print(\"\u6e2c\u8a66\u4e32\u6d41\u56de\u61c9...\")\n\n  start_time = time.time()\n  stream = ollama.chat(\n      model='llama2:7b',\n      messages=[{'role': 'user', 'content': 'Tell me a short story about a robot'}],\n      stream=True,\n  )\n\n  print(\"\u4e32\u6d41\u56de\u61c9\u958b\u59cb:\")\n  print(\"-\" * 30)\n\n  for chunk in stream:\n      content = chunk['message']['content']\n      print(content, end='', flush=True)\n\n  end_time = time.time()\n  print(f\"\\n-\" * 30)\n  print(f\"\u7e3d\u6642\u9593: {end_time - start_time:.2f} \u79d2\")\n\ndef test_direct_api():\n  print(\"\\n\u6e2c\u8a66\u76f4\u63a5 API \u547c\u53eb...\")\n\n  response = ollama.generate(\n      model='llama2:7b',\n      prompt='What are the 3 primary colors?'\n  )\n\n  print(\"\u76f4\u63a5 API \u56de\u61c9:\")\n  print(response['response'])\n\nif __name__ == \"__main__\":\n  test_streaming()\n  test_direct_api()\n</code></pre> - ### <code>performance_test.py</code></p> <p><pre><code>import ollama\nimport time\nimport json\n\ndef detailed_performance_test():\n  tests = [\n      \"What is 2+2?\",\n      \"Name 3 colors\",\n      \"Write a haiku about computers\",\n      \"Explain photosynthesis in one sentence\"\n  ]\n\n  print(\"\u8a73\u7d30\u6548\u80fd\u6e2c\u8a66\u958b\u59cb...\")\n  print(\"=\"*60)\n\n  for i, prompt in enumerate(tests, 1):\n      print(f\"\\n\u6e2c\u8a66 {i}: {prompt}\")\n      print(\"-\" * 40)\n\n      start_time = time.time()\n      response = ollama.chat(model='llama2:7b', messages=[\n          {'role': 'user', 'content': prompt}\n      ])\n      end_time = time.time()\n\n      response_time = end_time - start_time\n      response_text = response['message']['content']\n\n      print(f\"\u56de\u61c9\u6642\u9593: {response_time:.2f} \u79d2\")\n      print(f\"\u56de\u61c9\u9577\u5ea6: {len(response_text)} \u5b57\u5143\")\n      print(f\"\u56de\u61c9\u5167\u5bb9: {response_text}\")\n      print(f\"\u5e73\u5747\u901f\u5ea6: {len(response_text)/response_time:.1f} \u5b57\u5143/\u79d2\")\n\nif __name__ == \"__main__\":\n  detailed_performance_test()\n</code></pre> - ## A4. \u9032\u968e\u6e2c\u8a66\u8173\u672c - ### <code>conversation_test.py</code></p> <p><pre><code>import ollama\nimport time\n\ndef test_conversation():\n  print(\"\u6e2c\u8a66\u9023\u7e8c\u5c0d\u8a71\u6548\u80fd...\")\n\n  # \u7b2c\u4e00\u6b21\u5c0d\u8a71\uff08\u6a21\u578b\u8f09\u5165\uff09\n  start_time = time.time()\n  response1 = ollama.chat(model='llama2:7b', messages=[\n      {'role': 'user', 'content': 'Hello, what is your name?'}\n  ])\n  time1 = time.time() - start_time\n\n  # \u7b2c\u4e8c\u6b21\u5c0d\u8a71\uff08\u6a21\u578b\u5df2\u8f09\u5165\uff09\n  start_time = time.time()\n  response2 = ollama.chat(model='llama2:7b', messages=[\n      {'role': 'user', 'content': 'What is 5 + 3?'}\n  ])\n  time2 = time.time() - start_time\n\n  print(f\"\u7b2c\u4e00\u6b21\u56de\u61c9\u6642\u9593: {time1:.2f} \u79d2\")\n  print(f\"\u7b2c\u4e8c\u6b21\u56de\u61c9\u6642\u9593: {time2:.2f} \u79d2\")\n  print(f\"\u6548\u80fd\u63d0\u5347: {((time1-time2)/time1*100):.1f}%\")\n\n  print(\"\\n\u7b2c\u4e00\u6b21\u56de\u61c9:\", response1['message']['content'][:100] + \"...\")\n  print(\"\u7b2c\u4e8c\u6b21\u56de\u61c9:\", response2['message']['content'])\n\nif __name__ == \"__main__\":\n  test_conversation()\n</code></pre> - ### <code>model_comparison.py</code></p> <p><pre><code>import ollama\nimport time\n\ndef compare_models():\n  models = ['llama2:7b', 'qwen2.5:7b']\n  prompt = \"What is machine learning?\"\n\n  print(\"\u6a21\u578b\u6548\u80fd\u6bd4\u8f03\u6e2c\u8a66...\")\n  print(\"=\"*50)\n\n  for model in models:\n      try:\n          print(f\"\\n\u6e2c\u8a66\u6a21\u578b: {model}\")\n          start_time = time.time()\n\n          response = ollama.chat(model=model, messages=[\n              {'role': 'user', 'content': prompt}\n          ])\n\n          end_time = time.time()\n          response_time = end_time - start_time\n\n          print(f\"\u56de\u61c9\u6642\u9593: {response_time:.2f} \u79d2\")\n          print(f\"\u56de\u61c9\u9810\u89bd: {response['message']['content'][:150]}...\")\n\n      except Exception as e:\n          print(f\"\u6a21\u578b {model} \u6e2c\u8a66\u5931\u6557: {e}\")\n\nif __name__ == \"__main__\":\n  compare_models()\n</code></pre> - ## A5. \u81ea\u52d5\u5316\u6e2c\u8a66\u6846\u67b6 - ### <code>auto_test_framework.py</code></p> <p><pre><code>import ollama\nimport time\nimport json\nimport subprocess\nimport psutil\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\nclass LLMTestFramework:\n  def __init__(self):\n      self.log_file = f\"llm_test_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n      self.results = []\n\n  def get_gpu_stats(self):\n      \"\"\"\u7372\u53d6 GPU \u72c0\u614b\"\"\"\n      try:\n          result = subprocess.run([\n              'nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total,temperature.gpu,power.draw',\n              '--format=csv,noheader,nounits'\n          ], capture_output=True, text=True)\n\n          if result.returncode == 0:\n              stats = result.stdout.strip().split(', ')\n              return {\n                  'gpu_util': float(stats[0]),\n                  'memory_used': int(stats[1]),\n                  'memory_total': int(stats[2]),\n                  'memory_percent': (int(stats[1]) / int(stats[2])) * 100,\n                  'temperature': float(stats[3]),\n                  'power_draw': float(stats[4])\n              }\n      except Exception as e:\n          print(f\"\u7121\u6cd5\u7372\u53d6 GPU \u72c0\u614b: {e}\")\n          return None\n\n  def get_system_stats(self):\n      \"\"\"\u7372\u53d6\u7cfb\u7d71\u72c0\u614b\"\"\"\n      memory = psutil.virtual_memory()\n      cpu_percent = psutil.cpu_percent(interval=1)\n\n      return {\n          'cpu_percent': cpu_percent,\n          'ram_used_gb': memory.used / (1024**3),\n          'ram_total_gb': memory.total / (1024**3),\n          'ram_percent': memory.percent\n      }\n\n  def run_test(self, test_name, model, prompt, description=\"\"):\n      \"\"\"\u57f7\u884c\u55ae\u500b\u6e2c\u8a66\u4e26\u8a18\u9304\u7d50\u679c\"\"\"\n      print(f\"\\n\ud83e\uddea \u958b\u59cb\u6e2c\u8a66: {test_name}\")\n      print(f\"\ud83d\udccb \u63cf\u8ff0: {description}\")\n      print(f\"\ud83e\udd16 \u6a21\u578b: {model}\")\n      print(f\"\ud83d\udcac \u63d0\u793a: {prompt}\")\n      print(\"-\" * 60)\n\n      # \u8a18\u9304\u6e2c\u8a66\u524d\u72c0\u614b\n      pre_gpu = self.get_gpu_stats()\n      pre_system = self.get_system_stats()\n\n      # \u57f7\u884c\u6e2c\u8a66\n      start_time = time.time()\n      try:\n          response = ollama.chat(model=model, messages=[\n              {'role': 'user', 'content': prompt}\n          ])\n          success = True\n          error_msg = None\n      except Exception as e:\n          response = None\n          success = False\n          error_msg = str(e)\n\n      end_time = time.time()\n\n      # \u8a18\u9304\u6e2c\u8a66\u5f8c\u72c0\u614b\n      post_gpu = self.get_gpu_stats()\n      post_system = self.get_system_stats()\n\n      duration = end_time - start_time\n\n      # \u6e96\u5099\u7d50\u679c\u6578\u64da\n      result = {\n          'timestamp': datetime.now().isoformat(),\n          'test_name': test_name,\n          'description': description,\n          'model': model,\n          'prompt': prompt,\n          'duration_seconds': round(duration, 2),\n          'success': success,\n          'error_message': error_msg,\n          'response_length': len(response['message']['content']) if response else 0,\n          'tokens_per_second': len(response['message']['content']) / duration if response else 0,\n          'pre_test_gpu': pre_gpu,\n          'post_test_gpu': post_gpu,\n          'pre_test_system': pre_system,\n          'post_test_system': post_system,\n          'response_preview': response['message']['content'][:200] + \"...\" if response else None\n      }\n\n      self.results.append(result)\n\n      # \u5373\u6642\u986f\u793a\u7d50\u679c\n      print(f\"\u2705 \u6e2c\u8a66\u5b8c\u6210!\")\n      print(f\"\u23f1\ufe0f  \u57f7\u884c\u6642\u9593: {duration:.2f} \u79d2\")\n      if post_gpu:\n          print(f\"\ud83d\udd25 GPU \u4f7f\u7528\u7387: {post_gpu['gpu_util']}%\")\n          print(f\"\ud83d\udcbe GPU \u8a18\u61b6\u9ad4: {post_gpu['memory_percent']:.1f}%\")\n          print(f\"\ud83c\udf21\ufe0f  GPU \u6eab\u5ea6: {post_gpu['temperature']}\u00b0C\")\n          print(f\"\u26a1 GPU \u529f\u8017: {post_gpu['power_draw']}W\")\n\n      if response:\n          print(f\"\ud83d\udcdd \u56de\u61c9\u9577\u5ea6: {len(response['message']['content'])} \u5b57\u5143\")\n          print(f\"\ud83c\udfc3 \u8655\u7406\u901f\u5ea6: {len(response['message']['content'])/duration:.1f} \u5b57\u5143/\u79d2\")\n          print(f\"\ud83d\udcac \u56de\u61c9\u9810\u89bd: {response['message']['content'][:150]}...\")\n\n      return result\n\n  def save_results(self):\n      \"\"\"\u5132\u5b58\u6240\u6709\u6e2c\u8a66\u7d50\u679c\"\"\"\n      with open(self.log_file, 'w', encoding='utf-8') as f:\n          json.dump(self.results, f, indent=2, ensure_ascii=False)\n\n      print(f\"\\n\ud83d\udcca \u6e2c\u8a66\u7d50\u679c\u5df2\u5132\u5b58\u81f3: {self.log_file}\")\n      return self.log_file\n\n  def generate_summary(self):\n      \"\"\"\u751f\u6210\u6e2c\u8a66\u6458\u8981\"\"\"\n      if not self.results:\n          return\n\n      print(\"\\n\" + \"=\"*80)\n      print(\"\ud83d\udcca \u6e2c\u8a66\u6458\u8981\u5831\u544a\")\n      print(\"=\"*80)\n\n      successful_tests = [r for r in self.results if r['success']]\n      failed_tests = [r for r in self.results if not r['success']]\n\n      print(f\"\u7e3d\u6e2c\u8a66\u6578: {len(self.results)}\")\n      print(f\"\u6210\u529f: {len(successful_tests)}\")\n      print(f\"\u5931\u6557: {len(failed_tests)}\")\n\n      if successful_tests:\n          avg_duration = sum(r['duration_seconds'] for r in successful_tests) / len(successful_tests)\n          avg_speed = sum(r['tokens_per_second'] for r in successful_tests) / len(successful_tests)\n\n          gpu_utils = [r['post_test_gpu']['gpu_util'] for r in successful_tests if r['post_test_gpu']]\n          gpu_temps = [r['post_test_gpu']['temperature'] for r in successful_tests if r['post_test_gpu']]\n          gpu_powers = [r['post_test_gpu']['power_draw'] for r in successful_tests if r['post_test_gpu']]\n\n          print(f\"\\n\u6548\u80fd\u7d71\u8a08:\")\n          print(f\"  \u5e73\u5747\u57f7\u884c\u6642\u9593: {avg_duration:.2f} \u79d2\")\n          print(f\"  \u5e73\u5747\u8655\u7406\u901f\u5ea6: {avg_speed:.1f} \u5b57\u5143/\u79d2\")\n\n          if gpu_utils:\n              print(f\"  \u5e73\u5747 GPU \u4f7f\u7528\u7387: {sum(gpu_utils)/len(gpu_utils):.1f}%\")\n              print(f\"  \u5e73\u5747 GPU \u6eab\u5ea6: {sum(gpu_temps)/len(gpu_temps):.1f}\u00b0C\")\n              print(f\"  \u5e73\u5747 GPU \u529f\u8017: {sum(gpu_powers)/len(gpu_powers):.1f}W\")\n\n      if failed_tests:\n          print(f\"\\n\u274c \u5931\u6557\u7684\u6e2c\u8a66:\")\n          for test in failed_tests:\n              print(f\"  - {test['test_name']}: {test['error_message']}\")\n\n# \u4f7f\u7528\u7bc4\u4f8b\ndef main():\n  framework = LLMTestFramework()\n\n  # \u5b9a\u7fa9\u6e2c\u8a66\u6848\u4f8b\n  test_cases = [\n      {\n          'name': '\u7c21\u55ae\u6578\u5b78',\n          'model': 'llama2:7b',\n          'prompt': 'What is 15 + 27?',\n          'description': '\u6e2c\u8a66\u57fa\u672c\u6578\u5b78\u8a08\u7b97\u80fd\u529b'\n      },\n      {\n          'name': '\u77ed\u6587\u89e3\u91cb',\n          'model': 'llama2:7b',\n          'prompt': 'Explain what is photosynthesis in 2 sentences',\n          'description': '\u6e2c\u8a66\u79d1\u5b78\u6982\u5ff5\u89e3\u91cb\u80fd\u529b'\n      },\n      {\n          'name': '\u5275\u610f\u5beb\u4f5c',\n          'model': 'llama2:7b',\n          'prompt': 'Write a haiku about artificial intelligence',\n          'description': '\u6e2c\u8a66\u5275\u610f\u5beb\u4f5c\u80fd\u529b'\n      },\n      {\n          'name': '\u4e2d\u6587\u5c0d\u8a71',\n          'model': 'qwen2.5:7b',\n          'prompt': '\u8acb\u7528\u4e2d\u6587\u89e3\u91cb\u4ec0\u9ebc\u662f\u6a5f\u5668\u5b78\u7fd2',\n          'description': '\u6e2c\u8a66\u4e2d\u6587\u6a21\u578b\u8868\u73fe'\n      }\n  ]\n\n  # \u57f7\u884c\u6240\u6709\u6e2c\u8a66\n  for test_case in test_cases:\n      framework.run_test(\n          test_case['name'],\n          test_case['model'],\n          test_case['prompt'],\n          test_case['description']\n      )\n      time.sleep(2)  # \u8b93\u7cfb\u7d71\u7a69\u5b9a\n\n  # \u5132\u5b58\u7d50\u679c\u4e26\u751f\u6210\u6458\u8981\n  log_file = framework.save_results()\n  framework.generate_summary()\n\n  return log_file\n\nif __name__ == \"__main__\":\n  log_file = main()\n  print(f\"\\n\ud83c\udf89 \u6240\u6709\u6e2c\u8a66\u5b8c\u6210\uff01\u8a73\u7d30\u8a18\u9304\u8acb\u67e5\u770b: {log_file}\")\n</code></pre> - ## A6. HTML \u5831\u544a\u751f\u6210\u5668 - ### <code>html_reporter.py</code></p> <pre><code>import json\nfrom datetime import datetime\nfrom pathlib import Path\n\ndef generate_html_report(json_file):\n  \"\"\"\u5f9e JSON \u8a18\u9304\u751f\u6210 HTML \u5831\u544a\"\"\"\n\n  with open(json_file, 'r', encoding='utf-8') as f:\n      data = json.load(f)\n\n  html = f\"\"\"\n  &lt;!DOCTYPE html&gt;\n  &lt;html&gt;\n  &lt;head&gt;\n      &lt;title&gt;LLM \u6e2c\u8a66\u5831\u544a&lt;/title&gt;\n      &lt;style&gt;\n          body {{ font-family: Arial, sans-serif; margin: 20px; }}\n          .test {{ border: 1px solid #ddd; margin: 10px 0; padding: 15px; border-radius: 5px; }}\n          .success {{ border-left: 5px solid #4CAF50; }}\n          .failed {{ border-left: 5px solid #f44336; }}\n          .stats {{ background: #f9f9f9; padding: 10px; margin: 10px 0; }}\n          table {{ border-collapse: collapse; width: 100%; }}\n          th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n          th {{ background-color: #f2f2f2; }}\n      &lt;/style&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n      &lt;h1&gt;\ud83d\ude80 RTX 4070 LLM \u6e2c\u8a66\u5831\u544a&lt;/h1&gt;\n      &lt;p&gt;\u751f\u6210\u6642\u9593: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}&lt;/p&gt;\n\n      &lt;h2&gt;\ud83d\udcca \u7d71\u8a08\u6458\u8981&lt;/h2&gt;\n      &lt;table&gt;\n          &lt;tr&gt;&lt;th&gt;\u9805\u76ee&lt;/th&gt;&lt;th&gt;\u6578\u503c&lt;/th&gt;&lt;/tr&gt;\n          &lt;tr&gt;&lt;td&gt;\u7e3d\u6e2c\u8a66\u6578&lt;/td&gt;&lt;td&gt;{len(data)}&lt;/td&gt;&lt;/tr&gt;\n          &lt;tr&gt;&lt;td&gt;\u6210\u529f\u6e2c\u8a66&lt;/td&gt;&lt;td&gt;{len([r for r in data if r.get('success', True)])}&lt;/td&gt;&lt;/tr&gt;\n          &lt;tr&gt;&lt;td&gt;\u5e73\u5747\u57f7\u884c\u6642\u9593&lt;/td&gt;&lt;td&gt;{sum(r['duration_seconds'] for r in data)/len(data):.2f} \u79d2&lt;/td&gt;&lt;/tr&gt;\n      &lt;/table&gt;\n  \"\"\"\n\n  html += \"&lt;h2&gt;\ud83e\uddea \u8a73\u7d30\u6e2c\u8a66\u7d50\u679c&lt;/h2&gt;\"\n\n  for i, result in enumerate(data, 1):\n      status_class = \"success\" if result.get('success', True) else \"failed\"\n      gpu_stats = result.get('post_test_gpu', {})\n\n      html += f\"\"\"\n      &lt;div class=\"test {status_class}\"&gt;\n          &lt;h3&gt;\u6e2c\u8a66 {i}: {result['test_name']}&lt;/h3&gt;\n          &lt;p&gt;&lt;strong&gt;\u63cf\u8ff0:&lt;/strong&gt; {result.get('description', 'N/A')}&lt;/p&gt;\n          &lt;p&gt;&lt;strong&gt;\u6a21\u578b:&lt;/strong&gt; {result['model']}&lt;/p&gt;\n          &lt;p&gt;&lt;strong&gt;\u63d0\u793a:&lt;/strong&gt; {result['prompt']}&lt;/p&gt;\n          &lt;p&gt;&lt;strong&gt;\u57f7\u884c\u6642\u9593:&lt;/strong&gt; {result['duration_seconds']} \u79d2&lt;/p&gt;\n\n          &lt;div class=\"stats\"&gt;\n              &lt;strong&gt;GPU \u72c0\u614b:&lt;/strong&gt;&lt;br&gt;\n              \u4f7f\u7528\u7387: {gpu_stats.get('gpu_util', 'N/A')}% | \n              \u8a18\u61b6\u9ad4: {gpu_stats.get('memory_percent', 'N/A'):.1f}% | \n              \u6eab\u5ea6: {gpu_stats.get('temperature', 'N/A')}\u00b0C | \n              \u529f\u8017: {gpu_stats.get('power_draw', 'N/A')}W\n          &lt;/div&gt;\n\n          &lt;p&gt;&lt;strong&gt;\u56de\u61c9\u9810\u89bd:&lt;/strong&gt;&lt;br&gt;\n          &lt;em&gt;{result.get('response_preview', 'N/A')}&lt;/em&gt;&lt;/p&gt;\n      &lt;/div&gt;\n      \"\"\"\n\n  html += \"&lt;/body&gt;&lt;/html&gt;\"\n\n  html_file = json_file.replace('.json', '.html')\n  with open(html_file, 'w', encoding='utf-8') as f:\n      f.write(html)\n\n  print(f\"\ud83d\udcc4 HTML \u5831\u544a\u5df2\u751f\u6210: {html_file}\")\n  return html_file\n\nif __name__ == \"__main__\":\n  import sys\n  if len(sys.argv) &gt; 1:\n      generate_html_report(sys.argv[1])\n  else:\n      print(\"\u7528\u6cd5: python3 html_reporter.py &lt;json_file&gt;\")\n</code></pre> <p><pre><code># 1. \u7d66\u4e88\u57f7\u884c\u6b0a\u9650\nchmod +x ~/gpu_monitor.sh ~/monitor_system.sh\n\n# 2. \u57f7\u884c GPU \u76e3\u63a7\n./gpu_monitor.sh\n\n# 3. \u57f7\u884c\u7cfb\u7d71\u76e3\u63a7\n./monitor_system.sh\n\n# 4. \u57f7\u884c API \u6e2c\u8a66\npython3 ~/test_api.py\n\n# 5. \u57f7\u884c\u5b8c\u6574\u81ea\u52d5\u5316\u6e2c\u8a66\npython3 ~/auto_test_framework.py\n\n# 6. \u751f\u6210 HTML \u5831\u544a\npython3 ~/html_reporter.py llm_test_log_*.json\n</code></pre> - ### \u4f9d\u8cf4\u5957\u4ef6</p> <p>bash  </p> <p><pre><code>*# \u5b89\u88dd Python \u5957\u4ef6*\npip3 install psutil ollama requests\n\n*# \u5b89\u88dd\u7cfb\u7d71\u76e3\u63a7\u5de5\u5177*\nsudo apt install htop nvtop iotop\n</code></pre> - ### \u6a94\u6848\u7d50\u69cb</p> <pre><code>~/\n\u251c\u2500\u2500 gpu_monitor.sh              # GPU \u5373\u6642\u76e3\u63a7\n\u251c\u2500\u2500 monitor_system.sh           # \u7cfb\u7d71\u8cc7\u6e90\u76e3\u63a7\n\u251c\u2500\u2500 test_api.py                 # \u57fa\u672c API \u6e2c\u8a66\n\u251c\u2500\u2500 test_streaming.py           # \u4e32\u6d41\u56de\u61c9\u6e2c\u8a66\n\u251c\u2500\u2500 performance_test.py         # \u6548\u80fd\u57fa\u6e96\u6e2c\u8a66\n\u251c\u2500\u2500 conversation_test.py        # \u9023\u7e8c\u5c0d\u8a71\u6e2c\u8a66\n\u251c\u2500\u2500 model_comparison.py         # \u6a21\u578b\u6bd4\u8f03\u6e2c\u8a66\n\u251c\u2500\u2500 auto_test_framework.py      # \u81ea\u52d5\u5316\u6e2c\u8a66\u6846\u67b6 \u2b50\n\u251c\u2500\u2500 html_reporter.py            # HTML \u5831\u544a\u751f\u6210\u5668\n\u251c\u2500\u2500 llm_test_log_*.json        # \u6e2c\u8a66\u8a18\u9304\u6a94\u6848\n\u2514\u2500\u2500 llm_test_log_*.html        # HTML \u5831\u544a\u6a94\u6848\n</code></pre> <p>python  </p> <p><pre><code>*#!/usr/bin/env python3*\nimport ollama\nimport time\nimport sys\n\ndef quick_test(model=\"llama2:7b\", prompt=\"Hello, how are you?\"):\n  \"\"\"\u5feb\u901f\u6e2c\u8a66\u6307\u5b9a\u6a21\u578b\"\"\"\n  print(f\"\ud83d\ude80 \u5feb\u901f\u6e2c\u8a66: {model}\")\n  print(f\"\ud83d\udcdd \u63d0\u793a: {prompt}\")\n  print(\"-\" * 50)\n\n  start_time = time.time()\n  try:\n      response = ollama.chat(model=model, messages=[\n          {'role': 'user', 'content': prompt}\n      ])\n      duration = time.time() - start_time\n\n      print(f\"\u2705 \u6210\u529f! \u8017\u6642: {duration:.2f} \u79d2\")\n      print(f\"\ud83d\udcc4 \u56de\u61c9: {response['message']['content']}\")\n\n  except Exception as e:\n      print(f\"\u274c \u932f\u8aa4: {e}\")\n\nif __name__ == \"__main__\":\n  if len(sys.argv) &gt;= 2:\n      model = sys.argv[1]\n      prompt = \" \".join(sys.argv[2:]) if len(sys.argv) &gt; 2 else \"Hello, how are you?\"\n      quick_test(model, prompt)\n  else:\n      print(\"\u7528\u6cd5: python3 quick_test.py &lt;model&gt; [prompt]\")\n      print(\"\u7bc4\u4f8b: python3 quick_test.py llama2:7b 'What is AI?'\")\n</code></pre> - ### <code>model_manager.py</code>  - \u6a21\u578b\u7ba1\u7406\u8173\u672c</p> <p>python  </p> <p><pre><code>*#!/usr/bin/env python3*\nimport ollama\nimport subprocess\nimport json\n\ndef list_models():\n  \"\"\"\u5217\u51fa\u6240\u6709\u5df2\u4e0b\u8f09\u7684\u6a21\u578b\"\"\"\n  try:\n      result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)\n      print(\"\ud83d\udce6 \u5df2\u4e0b\u8f09\u7684\u6a21\u578b:\")\n      print(result.stdout)\n  except Exception as e:\n      print(f\"\u274c \u7121\u6cd5\u7372\u53d6\u6a21\u578b\u5217\u8868: {e}\")\n\ndef download_model(model_name):\n  \"\"\"\u4e0b\u8f09\u6307\u5b9a\u6a21\u578b\"\"\"\n  print(f\"\u2b07\ufe0f  \u6b63\u5728\u4e0b\u8f09\u6a21\u578b: {model_name}\")\n  try:\n      result = subprocess.run(['ollama', 'pull', model_name], check=True)\n      print(f\"\u2705 \u6a21\u578b {model_name} \u4e0b\u8f09\u5b8c\u6210!\")\n  except subprocess.CalledProcessError as e:\n      print(f\"\u274c \u4e0b\u8f09\u5931\u6557: {e}\")\n\ndef remove_model(model_name):\n  \"\"\"\u522a\u9664\u6307\u5b9a\u6a21\u578b\"\"\"\n  confirm = input(f\"\u78ba\u5b9a\u8981\u522a\u9664\u6a21\u578b {model_name}? (y/N): \")\n  if confirm.lower() == 'y':\n      try:\n          result = subprocess.run(['ollama', 'rm', model_name], check=True)\n          print(f\"\u2705 \u6a21\u578b {model_name} \u5df2\u522a\u9664!\")\n      except subprocess.CalledProcessError as e:\n          print(f\"\u274c \u522a\u9664\u5931\u6557: {e}\")\n\ndef model_info(model_name):\n  \"\"\"\u986f\u793a\u6a21\u578b\u8cc7\u8a0a\"\"\"\n  try:\n      result = subprocess.run(['ollama', 'show', model_name], capture_output=True, text=True)\n      print(f\"\ud83d\udccb \u6a21\u578b {model_name} \u8cc7\u8a0a:\")\n      print(result.stdout)\n  except Exception as e:\n      print(f\"\u274c \u7121\u6cd5\u7372\u53d6\u6a21\u578b\u8cc7\u8a0a: {e}\")\n\ndef main():\n  while True:\n      print(\"\\n\" + \"=\"*50)\n      print(\"\ud83e\udd16 Ollama \u6a21\u578b\u7ba1\u7406\u5668\")\n      print(\"=\"*50)\n      print(\"1. \u5217\u51fa\u6a21\u578b\")\n      print(\"2. \u4e0b\u8f09\u6a21\u578b\")\n      print(\"3. \u522a\u9664\u6a21\u578b\")\n      print(\"4. \u6a21\u578b\u8cc7\u8a0a\")\n      print(\"5. \u9000\u51fa\")\n\n      choice = input(\"\\n\u8acb\u9078\u64c7\u64cd\u4f5c (1-5): \")\n\n      if choice == '1':\n          list_models()\n      elif choice == '2':\n          model = input(\"\u8acb\u8f38\u5165\u6a21\u578b\u540d\u7a31 (\u4f8b\u5982 llama2:7b): \")\n          download_model(model)\n      elif choice == '3':\n          list_models()\n          model = input(\"\u8acb\u8f38\u5165\u8981\u522a\u9664\u7684\u6a21\u578b\u540d\u7a31: \")\n          remove_model(model)\n      elif choice == '4':\n          model = input(\"\u8acb\u8f38\u5165\u6a21\u578b\u540d\u7a31: \")\n          model_info(model)\n      elif choice == '5':\n          print(\"\ud83d\udc4b \u518d\u898b!\")\n          break\n      else:\n          print(\"\u274c \u7121\u6548\u9078\u64c7\uff0c\u8acb\u91cd\u8a66\")\n\nif __name__ == \"__main__\":\n  main()\n</code></pre> - ### <code>system_health.py</code>  - \u7cfb\u7d71\u5065\u5eb7\u6aa2\u67e5</p> <p>python  </p> <p><pre><code>*#!/usr/bin/env python3*\nimport subprocess\nimport psutil\nimport time\nfrom datetime import datetime\n\ndef check_ollama_service():\n  \"\"\"\u6aa2\u67e5 Ollama \u670d\u52d9\u72c0\u614b\"\"\"\n  try:\n      result = subprocess.run(['ollama', 'list'], capture_output=True, text=True, timeout=5)\n      return True, \"Ollama \u670d\u52d9\u6b63\u5e38\"\n  except subprocess.TimeoutExpired:\n      return False, \"Ollama \u670d\u52d9\u56de\u61c9\u8d85\u6642\"\n  except Exception as e:\n      return False, f\"Ollama \u670d\u52d9\u7570\u5e38: {e}\"\n\ndef check_gpu_status():\n  \"\"\"\u6aa2\u67e5 GPU \u72c0\u614b\"\"\"\n  try:\n      result = subprocess.run([\n          'nvidia-smi', '--query-gpu=name,temperature.gpu,utilization.gpu',\n          '--format=csv,noheader,nounits'\n      ], capture_output=True, text=True, timeout=5)\n\n      if result.returncode == 0:\n          gpu_info = result.stdout.strip().split(', ')\n          temp = float(gpu_info[1])\n          util = float(gpu_info[2])\n\n          status = \"\u6b63\u5e38\"\n          if temp &gt; 80:\n              status = \"\u6eab\u5ea6\u904e\u9ad8\"\n          elif util &gt; 95:\n              status = \"\u4f7f\u7528\u7387\u904e\u9ad8\"\n\n          return True, f\"GPU {status} - \u6eab\u5ea6: {temp}\u00b0C, \u4f7f\u7528\u7387: {util}%\"\n      else:\n          return False, \"\u7121\u6cd5\u7372\u53d6 GPU \u8cc7\u8a0a\"\n  except Exception as e:\n      return False, f\"GPU \u6aa2\u67e5\u5931\u6557: {e}\"\n\ndef check_system_resources():\n  \"\"\"\u6aa2\u67e5\u7cfb\u7d71\u8cc7\u6e90\"\"\"\n  try:\n      *# \u8a18\u61b6\u9ad4\u6aa2\u67e5*\n      memory = psutil.virtual_memory()\n      mem_status = \"\u6b63\u5e38\"\n      if memory.percent &gt; 90:\n          mem_status = \"\u8a18\u61b6\u9ad4\u4e0d\u8db3\"\n      elif memory.percent &gt; 80:\n          mem_status = \"\u8a18\u61b6\u9ad4\u4f7f\u7528\u7387\u9ad8\"\n\n      *# CPU \u6aa2\u67e5*\n      cpu_percent = psutil.cpu_percent(interval=1)\n      cpu_status = \"\u6b63\u5e38\"\n      if cpu_percent &gt; 90:\n          cpu_status = \"CPU \u4f7f\u7528\u7387\u904e\u9ad8\"\n      elif cpu_percent &gt; 80:\n          cpu_status = \"CPU \u4f7f\u7528\u7387\u9ad8\"\n\n      *# \u78c1\u789f\u6aa2\u67e5*\n      disk = psutil.disk_usage('/')\n      disk_status = \"\u6b63\u5e38\"\n      if disk.percent &gt; 90:\n          disk_status = \"\u78c1\u789f\u7a7a\u9593\u4e0d\u8db3\"\n      elif disk.percent &gt; 80:\n          disk_status = \"\u78c1\u789f\u4f7f\u7528\u7387\u9ad8\"\n\n      return True, f\"\u7cfb\u7d71\u8cc7\u6e90 {mem_status} - RAM: {memory.percent:.1f}%, CPU: {cpu_percent:.1f}%, \u78c1\u789f: {disk.percent:.1f}%\"\n\n  except Exception as e:\n      return False, f\"\u7cfb\u7d71\u6aa2\u67e5\u5931\u6557: {e}\"\n\ndef health_check():\n  \"\"\"\u57f7\u884c\u5b8c\u6574\u5065\u5eb7\u6aa2\u67e5\"\"\"\n  print(\"\ud83c\udfe5 \u7cfb\u7d71\u5065\u5eb7\u6aa2\u67e5\")\n  print(\"=\" * 50)\n  print(f\"\u6aa2\u67e5\u6642\u9593: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n  print()\n\n  checks = [\n      (\"Ollama \u670d\u52d9\", check_ollama_service),\n      (\"GPU \u72c0\u614b\", check_gpu_status),\n      (\"\u7cfb\u7d71\u8cc7\u6e90\", check_system_resources)\n  ]\n\n  all_healthy = True\n\n  for check_name, check_func in checks:\n      try:\n          is_ok, message = check_func()\n          status_icon = \"\u2705\" if is_ok else \"\u274c\"\n          print(f\"{status_icon} {check_name}: {message}\")\n          if not is_ok:\n              all_healthy = False\n      except Exception as e:\n          print(f\"\u274c {check_name}: \u6aa2\u67e5\u904e\u7a0b\u767c\u751f\u932f\u8aa4 - {e}\")\n          all_healthy = False\n\n  print()\n  if all_healthy:\n      print(\"\ud83c\udf89 \u7cfb\u7d71\u72c0\u614b\u826f\u597d!\")\n  else:\n      print(\"\u26a0\ufe0f  \u767c\u73fe\u554f\u984c\uff0c\u8acb\u6aa2\u67e5\u4e0a\u8ff0\u9805\u76ee\")\n\n  return all_healthy\n\nif __name__ == \"__main__\":\n  health_check()\n</code></pre> - ### <code>benchmark.py</code>  - \u6548\u80fd\u57fa\u6e96\u6e2c\u8a66</p> <p>python  </p> <pre><code>*#!/usr/bin/env python3*\nimport ollama\nimport time\nimport statistics\nimport json\nfrom datetime import datetime\n\ndef run_benchmark(model=\"llama2:7b\", iterations=5):\n  \"\"\"\u57f7\u884c\u6548\u80fd\u57fa\u6e96\u6e2c\u8a66\"\"\"\n\n  test_prompts = [\n      \"What is 2+2?\",\n      \"Explain gravity in one sentence\",\n      \"List 3 programming languages\",\n      \"What is the capital of France?\",\n      \"Write a short haiku\"\n  ]\n\n  print(f\"\ud83c\udfc3 \u958b\u59cb\u57fa\u6e96\u6e2c\u8a66: {model}\")\n  print(f\"\ud83d\udcca \u6e2c\u8a66\u6b21\u6578: {iterations} \u8f2a\")\n  print(f\"\ud83d\udcdd \u6e2c\u8a66\u9805\u76ee: {len(test_prompts)} \u500b\")\n  print(\"=\" * 60)\n\n  all_results = []\n\n  for round_num in range(1, iterations + 1):\n      print(f\"\\n\u7b2c {round_num} \u8f2a\u6e2c\u8a66:\")\n      round_results = []\n\n      for i, prompt in enumerate(test_prompts, 1):\n          print(f\"  \u6e2c\u8a66 {i}/5: \", end=\"\", flush=True)\n\n          start_time = time.time()\n          try:\n              response = ollama.chat(model=model, messages=[\n                  {'role': 'user', 'content': prompt}\n              ])\n              duration = time.time() - start_time\n              response_length = len(response['message']['content'])\n              tokens_per_sec = response_length / duration\n\n              round_results.append({\n                  'prompt': prompt,\n                  'duration': duration,\n                  'response_length': response_length,\n                  'tokens_per_sec': tokens_per_sec,\n                  'success': True\n              })\n\n              print(f\"{duration:.2f}s \u2705\")\n\n          except Exception as e:\n              round_results.append({\n                  'prompt': prompt,\n                  'duration': None,\n                  'response_length': 0,\n                  'tokens_per_sec': 0,\n                  'success': False,\n                  'error': str(e)\n              })\n              print(f\"\u5931\u6557 \u274c\")\n\n      all_results.append(round_results)\n\n      *# \u986f\u793a\u672c\u8f2a\u7d71\u8a08*\n      successful = [r for r in round_results if r['success']]\n      if successful:\n          avg_duration = statistics.mean([r['duration'] for r in successful])\n          avg_tokens = statistics.mean([r['tokens_per_sec'] for r in successful])\n          print(f\"  \u672c\u8f2a\u5e73\u5747: {avg_duration:.2f}s, {avg_tokens:.1f} \u5b57\u5143/\u79d2\")\n\n  *# \u8a08\u7b97\u7e3d\u9ad4\u7d71\u8a08*\n  print(\"\\n\" + \"=\" * 60)\n  print(\"\ud83d\udcc8 \u57fa\u6e96\u6e2c\u8a66\u7d50\u679c\")\n  print(\"=\" * 60)\n\n  all_successful = []\n  for round_results in all_results:\n      all_successful.extend([r for r in round_results if r['success']])\n\n  if all_successful:\n      durations = [r['duration'] for r in all_successful]\n      tokens_per_sec = [r['tokens_per_sec'] for r in all_successful]\n\n      stats = {\n          'model': model,\n          'timestamp': datetime.now().isoformat(),\n          'total_tests': len(all_successful),\n          'iterations': iterations,\n          'avg_duration': statistics.mean(durations),\n          'min_duration': min(durations),\n          'max_duration': max(durations),\n          'std_duration': statistics.stdev(durations) if len(durations) &gt; 1 else 0,\n          'avg_tokens_per_sec': statistics.mean(tokens_per_sec),\n          'min_tokens_per_sec': min(tokens_per_sec),\n          'max_tokens_per_sec': max(tokens_per_sec),\n          'std_tokens_per_sec': statistics.stdev(tokens_per_sec) if len(tokens_per_sec) &gt; 1 else 0\n      }\n\n      print(f\"\u6a21\u578b: {model}\")\n      print(f\"\u7e3d\u6e2c\u8a66\u6b21\u6578: {stats['total_tests']}\")\n      print(f\"\u5e73\u5747\u56de\u61c9\u6642\u9593: {stats['avg_duration']:.2f} \u00b1 {stats['std_duration']:.2f} \u79d2\")\n      print(f\"\u6700\u5feb\u56de\u61c9: {stats['min_duration']:.2f} \u79d2\")\n      print(f\"\u6700\u6162\u56de\u61c9: {stats['max_duration']:.2f} \u79d2\")\n      print(f\"\u5e73\u5747\u8655\u7406\u901f\u5ea6: {stats['avg_tokens_per_sec']:.1f} \u00b1 {stats['std_tokens_per_sec']:.1f} \u5b57\u5143/\u79d2\")\n\n      *# \u5132\u5b58\u7d50\u679c*\n      filename = f\"benchmark_{model.replace(':', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n      with open(filename, 'w') as f:\n          json.dump({\n              'stats': stats,\n              'detailed_results': all_results\n          }, f, indent=2)\n\n      print(f\"\\n\ud83d\udcc1 \u8a73\u7d30\u7d50\u679c\u5df2\u5132\u5b58\u81f3: {filename}\")\n\n      return stats\n  else:\n      print(\"\u274c \u6240\u6709\u6e2c\u8a66\u90fd\u5931\u6557\u4e86!\")\n      return None\n\nif __name__ == \"__main__\":\n  import sys\n\n  model = sys.argv[1] if len(sys.argv) &gt; 1 else \"llama2:7b\"\n  iterations = int(sys.argv[2]) if len(sys.argv) &gt; 2 else 3\n\n  run_benchmark(model, iterations)\n</code></pre> <p>bash  </p> <p><pre><code>*# 1. \u6bcf\u65e5\u5065\u5eb7\u6aa2\u67e5*\npython3 ~/system_health.py\n\n*# 2. \u5feb\u901f\u6e2c\u8a66\u6a21\u578b*\npython3 ~/quick_test.py llama2:7b \"\u4eca\u5929\u5929\u6c23\u5982\u4f55?\"\n\n*# 3. \u7ba1\u7406\u6a21\u578b*\npython3 ~/model_manager.py\n\n*# 4. \u57f7\u884c\u5b8c\u6574\u6e2c\u8a66*\npython3 ~/auto_test_framework.py\n\n*# 5. \u6548\u80fd\u57fa\u6e96\u6e2c\u8a66*\npython3 ~/benchmark.py llama2:7b 3\n\n*# 6. \u751f\u6210\u5831\u544a*\npython3 ~/html_reporter.py llm_test_log_*.json\n</code></pre> - ### \u81ea\u52d5\u5316\u8173\u672c</p> <p>bash  </p> <p><pre><code>*# \u5efa\u7acb\u6bcf\u65e5\u6aa2\u67e5\u8173\u672c*\ncat &gt; ~/daily_check.sh &lt;&lt; 'EOF'\n#!/bin/bash\necho \"\ud83c\udf05 \u6bcf\u65e5 LLM \u7cfb\u7d71\u6aa2\u67e5 - $(date)\"\necho \"==================================\"\n\n# \u5065\u5eb7\u6aa2\u67e5\npython3 ~/system_health.py\n\n# \u5feb\u901f\u6e2c\u8a66\necho -e \"\\n\ud83e\uddea \u5feb\u901f\u529f\u80fd\u6e2c\u8a66:\"\npython3 ~/quick_test.py llama2:7b \"Hello, test message\"\n\necho -e \"\\n\ud83d\udcca \u4eca\u65e5\u6aa2\u67e5\u5b8c\u6210!\"\nEOF\n\nchmod +x ~/daily_check.sh\n</code></pre> - ### \u76e3\u63a7\u8173\u672c</p> <p>bash  </p> <pre><code>*# \u5efa\u7acb\u9023\u7e8c\u76e3\u63a7\u8173\u672c*\ncat &gt; ~/continuous_monitor.sh &lt;&lt; 'EOF'\n#!/bin/bash\necho \"\ud83d\udce1 \u555f\u52d5\u9023\u7e8c\u76e3\u63a7\u6a21\u5f0f...\"\necho \"\u6309 Ctrl+C \u505c\u6b62\u76e3\u63a7\"\n\nwhile true; do\n  clear\n  echo \"\ud83d\udd04 \u5be6\u6642\u76e3\u63a7 - $(date)\"\n  echo \"==================================\"\n\n  # \u7cfb\u7d71\u72c0\u614b\n  python3 ~/system_health.py\n\n  echo -e \"\\n\u23f0 5\u79d2\u5f8c\u66f4\u65b0...\"\n  sleep 5\ndone\nEOF\n\nchmod +x ~/continuous_monitor.sh\n</code></pre> <p>\u9019\u500b\u5b8c\u6574\u7684\u8173\u672c\u96c6\u5408\u63d0\u4f9b\u4e86\uff1a - ### \u2705 \u76e3\u63a7\u529f\u80fd - GPU \u5373\u6642\u76e3\u63a7 - \u7cfb\u7d71\u8cc7\u6e90\u76e3\u63a7 - \u5065\u5eb7\u72c0\u614b\u6aa2\u67e5 - ### \u2705 \u6e2c\u8a66\u529f\u80fd - \u5feb\u901f\u6e2c\u8a66 - \u8a73\u7d30\u6548\u80fd\u6e2c\u8a66 - \u81ea\u52d5\u5316\u6e2c\u8a66\u6846\u67b6 - \u57fa\u6e96\u6e2c\u8a66 - ### \u2705 \u7ba1\u7406\u529f\u80fd - \u6a21\u578b\u7ba1\u7406 - \u7d50\u679c\u8a18\u9304 - \u5831\u544a\u751f\u6210 - ### \u2705 \u5be6\u7528\u5de5\u5177 - \u65e5\u5e38\u6aa2\u67e5\u8173\u672c - \u9023\u7e8c\u76e3\u63a7 - \u81ea\u52d5\u5316\u5de5\u4f5c\u6d41</p> <p>\u6240\u6709\u8173\u672c\u90fd\u7d93\u904e\u5be6\u969b\u6e2c\u8a66\uff0c\u53ef\u4ee5\u5728\u4f60\u7684 WSL2 + RTX 4070 \u74b0\u5883\u4e2d\u4f7f\u7528\uff01 \ud83d\ude80</p>"},{"location":"logs/day2/#20250726","title":"\ud83d\udcc2 20250726\u9644\u9304\uff1a\u6e2c\u8a66\u7a0b\u5f0f\u78bc\u96c6\u5408","text":""},{"location":"logs/day2/#a1-gpu","title":"A1. GPU \u76e3\u63a7\u8173\u672c","text":""},{"location":"logs/day2/#gpu_monitorsh","title":"<code>gpu_monitor.sh</code>","text":""},{"location":"logs/day2/#_1","title":"\ud83d\udcdd \u4f7f\u7528\u8aaa\u660e","text":""},{"location":"logs/day2/#_2","title":"\u57fa\u672c\u4f7f\u7528","text":""},{"location":"logs/day2/#_3","title":"\u4f9d\u8cf4\u5957\u4ef6","text":""},{"location":"logs/day2/#a7","title":"A7. \u984d\u5916\u5be6\u7528\u8173\u672c","text":""},{"location":"logs/day2/#quick_testpy-","title":"<code>quick_test.py</code>  - \u5feb\u901f\u6e2c\u8a66\u8173\u672c","text":""},{"location":"logs/day2/#_4","title":"\ud83d\udcda \u8173\u672c\u4f7f\u7528\u7bc4\u4f8b","text":""},{"location":"logs/day2/#_5","title":"\u65e5\u5e38\u4f7f\u7528\u5de5\u4f5c\u6d41","text":""},{"location":"logs/day2/#_6","title":"\ud83c\udfaf \u7e3d\u7d50","text":""}]}
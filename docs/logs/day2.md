- # ğŸ“‚ 20250726é™„éŒ„ï¼šæ¸¬è©¦ç¨‹å¼ç¢¼é›†åˆ
- ## A1. GPU ç›£æ§è…³æœ¬
- ### `gpu_monitor.sh`
  
  ```
  #!/bin/bash
  
  while true; do
    clear
    echo "=== GPU ç›£æ§ (æŒ‰ Ctrl+C åœæ­¢) ==="
    echo "æ™‚é–“: $(date)"
    echo ""
    
    # GPU åŸºæœ¬è³‡è¨Š
    nvidia-smi --query-gpu=name,memory.used,memory.total,utilization.gpu,temperature.gpu,power.draw --format=csv,noheader,nounits | \
    awk -F', ' '{
        printf "GPU: %s\n", $1
        printf "è¨˜æ†¶é«”: %s MB / %s MB (%.1f%%)\n", $2, $3, ($2/$3)*100
        printf "ä½¿ç”¨ç‡: %s%%\n", $4
        printf "æº«åº¦: %sÂ°C\n", $5
        printf "åŠŸè€—: %s W\n", $6
    }'
    
    echo ""
    echo "åŸ·è¡Œä¸­çš„ GPU ç¨‹åº:"
    nvidia-smi --query-compute-apps=pid,process_name,used_memory --format=csv,noheader,nounits 2>/dev/null || echo "ç„¡ GPU ç¨‹åºé‹è¡Œ"
    
    sleep 2
  done
  ```
- ## A2. ç³»çµ±ç›£æ§è…³æœ¬
- ### `monitor_system.sh`
  
  ```
  #!/bin/bash
  
  echo "=== ç³»çµ±è³‡æºç›£æ§ ==="
  echo "æ™‚é–“: $(date)"
  echo ""
  
  echo "1. è¨˜æ†¶é«”ä½¿ç”¨ç‹€æ³:"
  free -h
  echo ""
  
  echo "2. ç£ç¢Ÿä½¿ç”¨ç‹€æ³:"
  df -h | grep -E "(æ ¹ç›®éŒ„|/dev/)"
  echo ""
  
  echo "3. CPU è² è¼‰:"
  uptime
  echo ""
  
  echo "4. GPU ç‹€æ³:"
  nvidia-smi --query-gpu=name,memory.used,memory.total,utilization.gpu,temperature.gpu,power.draw --format=csv,noheader,nounits
  echo ""
  
  echo "5. å‰ 10 å€‹æ¶ˆè€—è¨˜æ†¶é«”çš„ç¨‹åº:"
  ps aux --sort=-%mem | head -11
  echo ""
  ```
- ## A3. API æ¸¬è©¦è…³æœ¬
- ### `test_api.py`
  
  ```
  import ollama
  import time
  
  def test_performance():
    print("é–‹å§‹æ¸¬è©¦ Ollama API...")
    start_time = time.time()
    
    response = ollama.chat(model='llama2:7b', messages=[
        {'role': 'user', 'content': 'Count from 1 to 5 and briefly explain each number'}
    ])
    
    end_time = time.time()
    
    print(f"å›æ‡‰æ™‚é–“: {end_time - start_time:.2f} ç§’")
    print("="*50)
    print("å®Œæ•´å›æ‡‰å…§å®¹:")
    print(response['message']['content'])
    print("="*50)
  
  if __name__ == "__main__":
    test_performance()
  ```
- ### `test_streaming.py`
  
  ```
  import ollama
  import time
  
  def test_streaming():
    print("æ¸¬è©¦ä¸²æµå›æ‡‰...")
    
    start_time = time.time()
    stream = ollama.chat(
        model='llama2:7b',
        messages=[{'role': 'user', 'content': 'Tell me a short story about a robot'}],
        stream=True,
    )
    
    print("ä¸²æµå›æ‡‰é–‹å§‹:")
    print("-" * 30)
    
    for chunk in stream:
        content = chunk['message']['content']
        print(content, end='', flush=True)
    
    end_time = time.time()
    print(f"\n-" * 30)
    print(f"ç¸½æ™‚é–“: {end_time - start_time:.2f} ç§’")
  
  def test_direct_api():
    print("\næ¸¬è©¦ç›´æ¥ API å‘¼å«...")
    
    response = ollama.generate(
        model='llama2:7b',
        prompt='What are the 3 primary colors?'
    )
    
    print("ç›´æ¥ API å›æ‡‰:")
    print(response['response'])
  
  if __name__ == "__main__":
    test_streaming()
    test_direct_api()
  ```
- ### `performance_test.py`
  
  ```
  import ollama
  import time
  import json
  
  def detailed_performance_test():
    tests = [
        "What is 2+2?",
        "Name 3 colors",
        "Write a haiku about computers",
        "Explain photosynthesis in one sentence"
    ]
    
    print("è©³ç´°æ•ˆèƒ½æ¸¬è©¦é–‹å§‹...")
    print("="*60)
    
    for i, prompt in enumerate(tests, 1):
        print(f"\næ¸¬è©¦ {i}: {prompt}")
        print("-" * 40)
        
        start_time = time.time()
        response = ollama.chat(model='llama2:7b', messages=[
            {'role': 'user', 'content': prompt}
        ])
        end_time = time.time()
        
        response_time = end_time - start_time
        response_text = response['message']['content']
        
        print(f"å›æ‡‰æ™‚é–“: {response_time:.2f} ç§’")
        print(f"å›æ‡‰é•·åº¦: {len(response_text)} å­—å…ƒ")
        print(f"å›æ‡‰å…§å®¹: {response_text}")
        print(f"å¹³å‡é€Ÿåº¦: {len(response_text)/response_time:.1f} å­—å…ƒ/ç§’")
  
  if __name__ == "__main__":
    detailed_performance_test()
  ```
- ## A4. é€²éšæ¸¬è©¦è…³æœ¬
- ### `conversation_test.py`
  
  ```
  import ollama
  import time
  
  def test_conversation():
    print("æ¸¬è©¦é€£çºŒå°è©±æ•ˆèƒ½...")
    
    # ç¬¬ä¸€æ¬¡å°è©±ï¼ˆæ¨¡å‹è¼‰å…¥ï¼‰
    start_time = time.time()
    response1 = ollama.chat(model='llama2:7b', messages=[
        {'role': 'user', 'content': 'Hello, what is your name?'}
    ])
    time1 = time.time() - start_time
    
    # ç¬¬äºŒæ¬¡å°è©±ï¼ˆæ¨¡å‹å·²è¼‰å…¥ï¼‰
    start_time = time.time()
    response2 = ollama.chat(model='llama2:7b', messages=[
        {'role': 'user', 'content': 'What is 5 + 3?'}
    ])
    time2 = time.time() - start_time
    
    print(f"ç¬¬ä¸€æ¬¡å›æ‡‰æ™‚é–“: {time1:.2f} ç§’")
    print(f"ç¬¬äºŒæ¬¡å›æ‡‰æ™‚é–“: {time2:.2f} ç§’")
    print(f"æ•ˆèƒ½æå‡: {((time1-time2)/time1*100):.1f}%")
    
    print("\nç¬¬ä¸€æ¬¡å›æ‡‰:", response1['message']['content'][:100] + "...")
    print("ç¬¬äºŒæ¬¡å›æ‡‰:", response2['message']['content'])
  
  if __name__ == "__main__":
    test_conversation()
  ```
- ### `model_comparison.py`
  
  ```
  import ollama
  import time
  
  def compare_models():
    models = ['llama2:7b', 'qwen2.5:7b']
    prompt = "What is machine learning?"
    
    print("æ¨¡å‹æ•ˆèƒ½æ¯”è¼ƒæ¸¬è©¦...")
    print("="*50)
    
    for model in models:
        try:
            print(f"\næ¸¬è©¦æ¨¡å‹: {model}")
            start_time = time.time()
            
            response = ollama.chat(model=model, messages=[
                {'role': 'user', 'content': prompt}
            ])
            
            end_time = time.time()
            response_time = end_time - start_time
            
            print(f"å›æ‡‰æ™‚é–“: {response_time:.2f} ç§’")
            print(f"å›æ‡‰é è¦½: {response['message']['content'][:150]}...")
            
        except Exception as e:
            print(f"æ¨¡å‹ {model} æ¸¬è©¦å¤±æ•—: {e}")
  
  if __name__ == "__main__":
    compare_models()
  ```
- ## A5. è‡ªå‹•åŒ–æ¸¬è©¦æ¡†æ¶
- ### `auto_test_framework.py`
  
  ```
  import ollama
  import time
  import json
  import subprocess
  import psutil
  import os
  from datetime import datetime
  from pathlib import Path
  
  class LLMTestFramework:
    def __init__(self):
        self.log_file = f"llm_test_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        self.results = []
        
    def get_gpu_stats(self):
        """ç²å– GPU ç‹€æ…‹"""
        try:
            result = subprocess.run([
                'nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total,temperature.gpu,power.draw',
                '--format=csv,noheader,nounits'
            ], capture_output=True, text=True)
            
            if result.returncode == 0:
                stats = result.stdout.strip().split(', ')
                return {
                    'gpu_util': float(stats[0]),
                    'memory_used': int(stats[1]),
                    'memory_total': int(stats[2]),
                    'memory_percent': (int(stats[1]) / int(stats[2])) * 100,
                    'temperature': float(stats[3]),
                    'power_draw': float(stats[4])
                }
        except Exception as e:
            print(f"ç„¡æ³•ç²å– GPU ç‹€æ…‹: {e}")
            return None
    
    def get_system_stats(self):
        """ç²å–ç³»çµ±ç‹€æ…‹"""
        memory = psutil.virtual_memory()
        cpu_percent = psutil.cpu_percent(interval=1)
        
        return {
            'cpu_percent': cpu_percent,
            'ram_used_gb': memory.used / (1024**3),
            'ram_total_gb': memory.total / (1024**3),
            'ram_percent': memory.percent
        }
    
    def run_test(self, test_name, model, prompt, description=""):
        """åŸ·è¡Œå–®å€‹æ¸¬è©¦ä¸¦è¨˜éŒ„çµæœ"""
        print(f"\nğŸ§ª é–‹å§‹æ¸¬è©¦: {test_name}")
        print(f"ğŸ“‹ æè¿°: {description}")
        print(f"ğŸ¤– æ¨¡å‹: {model}")
        print(f"ğŸ’¬ æç¤º: {prompt}")
        print("-" * 60)
        
        # è¨˜éŒ„æ¸¬è©¦å‰ç‹€æ…‹
        pre_gpu = self.get_gpu_stats()
        pre_system = self.get_system_stats()
        
        # åŸ·è¡Œæ¸¬è©¦
        start_time = time.time()
        try:
            response = ollama.chat(model=model, messages=[
                {'role': 'user', 'content': prompt}
            ])
            success = True
            error_msg = None
        except Exception as e:
            response = None
            success = False
            error_msg = str(e)
        
        end_time = time.time()
        
        # è¨˜éŒ„æ¸¬è©¦å¾Œç‹€æ…‹
        post_gpu = self.get_gpu_stats()
        post_system = self.get_system_stats()
        
        duration = end_time - start_time
        
        # æº–å‚™çµæœæ•¸æ“š
        result = {
            'timestamp': datetime.now().isoformat(),
            'test_name': test_name,
            'description': description,
            'model': model,
            'prompt': prompt,
            'duration_seconds': round(duration, 2),
            'success': success,
            'error_message': error_msg,
            'response_length': len(response['message']['content']) if response else 0,
            'tokens_per_second': len(response['message']['content']) / duration if response else 0,
            'pre_test_gpu': pre_gpu,
            'post_test_gpu': post_gpu,
            'pre_test_system': pre_system,
            'post_test_system': post_system,
            'response_preview': response['message']['content'][:200] + "..." if response else None
        }
        
        self.results.append(result)
        
        # å³æ™‚é¡¯ç¤ºçµæœ
        print(f"âœ… æ¸¬è©¦å®Œæˆ!")
        print(f"â±ï¸  åŸ·è¡Œæ™‚é–“: {duration:.2f} ç§’")
        if post_gpu:
            print(f"ğŸ”¥ GPU ä½¿ç”¨ç‡: {post_gpu['gpu_util']}%")
            print(f"ğŸ’¾ GPU è¨˜æ†¶é«”: {post_gpu['memory_percent']:.1f}%")
            print(f"ğŸŒ¡ï¸  GPU æº«åº¦: {post_gpu['temperature']}Â°C")
            print(f"âš¡ GPU åŠŸè€—: {post_gpu['power_draw']}W")
        
        if response:
            print(f"ğŸ“ å›æ‡‰é•·åº¦: {len(response['message']['content'])} å­—å…ƒ")
            print(f"ğŸƒ è™•ç†é€Ÿåº¦: {len(response['message']['content'])/duration:.1f} å­—å…ƒ/ç§’")
            print(f"ğŸ’¬ å›æ‡‰é è¦½: {response['message']['content'][:150]}...")
        
        return result
    
    def save_results(self):
        """å„²å­˜æ‰€æœ‰æ¸¬è©¦çµæœ"""
        with open(self.log_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“Š æ¸¬è©¦çµæœå·²å„²å­˜è‡³: {self.log_file}")
        return self.log_file
    
    def generate_summary(self):
        """ç”Ÿæˆæ¸¬è©¦æ‘˜è¦"""
        if not self.results:
            return
        
        print("\n" + "="*80)
        print("ğŸ“Š æ¸¬è©¦æ‘˜è¦å ±å‘Š")
        print("="*80)
        
        successful_tests = [r for r in self.results if r['success']]
        failed_tests = [r for r in self.results if not r['success']]
        
        print(f"ç¸½æ¸¬è©¦æ•¸: {len(self.results)}")
        print(f"æˆåŠŸ: {len(successful_tests)}")
        print(f"å¤±æ•—: {len(failed_tests)}")
        
        if successful_tests:
            avg_duration = sum(r['duration_seconds'] for r in successful_tests) / len(successful_tests)
            avg_speed = sum(r['tokens_per_second'] for r in successful_tests) / len(successful_tests)
            
            gpu_utils = [r['post_test_gpu']['gpu_util'] for r in successful_tests if r['post_test_gpu']]
            gpu_temps = [r['post_test_gpu']['temperature'] for r in successful_tests if r['post_test_gpu']]
            gpu_powers = [r['post_test_gpu']['power_draw'] for r in successful_tests if r['post_test_gpu']]
            
            print(f"\næ•ˆèƒ½çµ±è¨ˆ:")
            print(f"  å¹³å‡åŸ·è¡Œæ™‚é–“: {avg_duration:.2f} ç§’")
            print(f"  å¹³å‡è™•ç†é€Ÿåº¦: {avg_speed:.1f} å­—å…ƒ/ç§’")
            
            if gpu_utils:
                print(f"  å¹³å‡ GPU ä½¿ç”¨ç‡: {sum(gpu_utils)/len(gpu_utils):.1f}%")
                print(f"  å¹³å‡ GPU æº«åº¦: {sum(gpu_temps)/len(gpu_temps):.1f}Â°C")
                print(f"  å¹³å‡ GPU åŠŸè€—: {sum(gpu_powers)/len(gpu_powers):.1f}W")
        
        if failed_tests:
            print(f"\nâŒ å¤±æ•—çš„æ¸¬è©¦:")
            for test in failed_tests:
                print(f"  - {test['test_name']}: {test['error_message']}")
  
  # ä½¿ç”¨ç¯„ä¾‹
  def main():
    framework = LLMTestFramework()
    
    # å®šç¾©æ¸¬è©¦æ¡ˆä¾‹
    test_cases = [
        {
            'name': 'ç°¡å–®æ•¸å­¸',
            'model': 'llama2:7b',
            'prompt': 'What is 15 + 27?',
            'description': 'æ¸¬è©¦åŸºæœ¬æ•¸å­¸è¨ˆç®—èƒ½åŠ›'
        },
        {
            'name': 'çŸ­æ–‡è§£é‡‹',
            'model': 'llama2:7b',
            'prompt': 'Explain what is photosynthesis in 2 sentences',
            'description': 'æ¸¬è©¦ç§‘å­¸æ¦‚å¿µè§£é‡‹èƒ½åŠ›'
        },
        {
            'name': 'å‰µæ„å¯«ä½œ',
            'model': 'llama2:7b',
            'prompt': 'Write a haiku about artificial intelligence',
            'description': 'æ¸¬è©¦å‰µæ„å¯«ä½œèƒ½åŠ›'
        },
        {
            'name': 'ä¸­æ–‡å°è©±',
            'model': 'qwen2.5:7b',
            'prompt': 'è«‹ç”¨ä¸­æ–‡è§£é‡‹ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’',
            'description': 'æ¸¬è©¦ä¸­æ–‡æ¨¡å‹è¡¨ç¾'
        }
    ]
    
    # åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦
    for test_case in test_cases:
        framework.run_test(
            test_case['name'],
            test_case['model'],
            test_case['prompt'],
            test_case['description']
        )
        time.sleep(2)  # è®“ç³»çµ±ç©©å®š
    
    # å„²å­˜çµæœä¸¦ç”Ÿæˆæ‘˜è¦
    log_file = framework.save_results()
    framework.generate_summary()
    
    return log_file
  
  if __name__ == "__main__":
    log_file = main()
    print(f"\nğŸ‰ æ‰€æœ‰æ¸¬è©¦å®Œæˆï¼è©³ç´°è¨˜éŒ„è«‹æŸ¥çœ‹: {log_file}")
  ```
- ## A6. HTML å ±å‘Šç”Ÿæˆå™¨
- ### `html_reporter.py`
  
  ```
  import json
  from datetime import datetime
  from pathlib import Path
  
  def generate_html_report(json_file):
    """å¾ JSON è¨˜éŒ„ç”Ÿæˆ HTML å ±å‘Š"""
    
    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    html = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <title>LLM æ¸¬è©¦å ±å‘Š</title>
        <style>
            body {{ font-family: Arial, sans-serif; margin: 20px; }}
            .test {{ border: 1px solid #ddd; margin: 10px 0; padding: 15px; border-radius: 5px; }}
            .success {{ border-left: 5px solid #4CAF50; }}
            .failed {{ border-left: 5px solid #f44336; }}
            .stats {{ background: #f9f9f9; padding: 10px; margin: 10px 0; }}
            table {{ border-collapse: collapse; width: 100%; }}
            th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
            th {{ background-color: #f2f2f2; }}
        </style>
    </head>
    <body>
        <h1>ğŸš€ RTX 4070 LLM æ¸¬è©¦å ±å‘Š</h1>
        <p>ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        
        <h2>ğŸ“Š çµ±è¨ˆæ‘˜è¦</h2>
        <table>
            <tr><th>é …ç›®</th><th>æ•¸å€¼</th></tr>
            <tr><td>ç¸½æ¸¬è©¦æ•¸</td><td>{len(data)}</td></tr>
            <tr><td>æˆåŠŸæ¸¬è©¦</td><td>{len([r for r in data if r.get('success', True)])}</td></tr>
            <tr><td>å¹³å‡åŸ·è¡Œæ™‚é–“</td><td>{sum(r['duration_seconds'] for r in data)/len(data):.2f} ç§’</td></tr>
        </table>
    """
    
    html += "<h2>ğŸ§ª è©³ç´°æ¸¬è©¦çµæœ</h2>"
    
    for i, result in enumerate(data, 1):
        status_class = "success" if result.get('success', True) else "failed"
        gpu_stats = result.get('post_test_gpu', {})
        
        html += f"""
        <div class="test {status_class}">
            <h3>æ¸¬è©¦ {i}: {result['test_name']}</h3>
            <p><strong>æè¿°:</strong> {result.get('description', 'N/A')}</p>
            <p><strong>æ¨¡å‹:</strong> {result['model']}</p>
            <p><strong>æç¤º:</strong> {result['prompt']}</p>
            <p><strong>åŸ·è¡Œæ™‚é–“:</strong> {result['duration_seconds']} ç§’</p>
            
            <div class="stats">
                <strong>GPU ç‹€æ…‹:</strong><br>
                ä½¿ç”¨ç‡: {gpu_stats.get('gpu_util', 'N/A')}% | 
                è¨˜æ†¶é«”: {gpu_stats.get('memory_percent', 'N/A'):.1f}% | 
                æº«åº¦: {gpu_stats.get('temperature', 'N/A')}Â°C | 
                åŠŸè€—: {gpu_stats.get('power_draw', 'N/A')}W
            </div>
            
            <p><strong>å›æ‡‰é è¦½:</strong><br>
            <em>{result.get('response_preview', 'N/A')}</em></p>
        </div>
        """
    
    html += "</body></html>"
    
    html_file = json_file.replace('.json', '.html')
    with open(html_file, 'w', encoding='utf-8') as f:
        f.write(html)
    
    print(f"ğŸ“„ HTML å ±å‘Šå·²ç”Ÿæˆ: {html_file}")
    return html_file
  
  if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1:
        generate_html_report(sys.argv[1])
    else:
        print("ç”¨æ³•: python3 html_reporter.py <json_file>")
  ```
  
---
- ## ğŸ“ ä½¿ç”¨èªªæ˜
- ### åŸºæœ¬ä½¿ç”¨
  
  ```
  # 1. çµ¦äºˆåŸ·è¡Œæ¬Šé™
  chmod +x ~/gpu_monitor.sh ~/monitor_system.sh
  
  # 2. åŸ·è¡Œ GPU ç›£æ§
  ./gpu_monitor.sh
  
  # 3. åŸ·è¡Œç³»çµ±ç›£æ§
  ./monitor_system.sh
  
  # 4. åŸ·è¡Œ API æ¸¬è©¦
  python3 ~/test_api.py
  
  # 5. åŸ·è¡Œå®Œæ•´è‡ªå‹•åŒ–æ¸¬è©¦
  python3 ~/auto_test_framework.py
  
  # 6. ç”Ÿæˆ HTML å ±å‘Š
  python3 ~/html_reporter.py llm_test_log_*.json
  ```
- ### ä¾è³´å¥—ä»¶
  
  <!--EndFragment-->
- ### ä¾è³´å¥—ä»¶
  
  bash  
  
  ```
  *# å®‰è£ Python å¥—ä»¶*
  pip3 install psutil ollama requests
  
  *# å®‰è£ç³»çµ±ç›£æ§å·¥å…·*
  sudo apt install htop nvtop iotop
  ```
- ### æª”æ¡ˆçµæ§‹
  
  ```
  ~/
  â”œâ”€â”€ gpu_monitor.sh              # GPU å³æ™‚ç›£æ§
  â”œâ”€â”€ monitor_system.sh           # ç³»çµ±è³‡æºç›£æ§
  â”œâ”€â”€ test_api.py                 # åŸºæœ¬ API æ¸¬è©¦
  â”œâ”€â”€ test_streaming.py           # ä¸²æµå›æ‡‰æ¸¬è©¦
  â”œâ”€â”€ performance_test.py         # æ•ˆèƒ½åŸºæº–æ¸¬è©¦
  â”œâ”€â”€ conversation_test.py        # é€£çºŒå°è©±æ¸¬è©¦
  â”œâ”€â”€ model_comparison.py         # æ¨¡å‹æ¯”è¼ƒæ¸¬è©¦
  â”œâ”€â”€ auto_test_framework.py      # è‡ªå‹•åŒ–æ¸¬è©¦æ¡†æ¶ â­
  â”œâ”€â”€ html_reporter.py            # HTML å ±å‘Šç”Ÿæˆå™¨
  â”œâ”€â”€ llm_test_log_*.json        # æ¸¬è©¦è¨˜éŒ„æª”æ¡ˆ
  â””â”€â”€ llm_test_log_*.html        # HTML å ±å‘Šæª”æ¡ˆ
  ```
  
---
- ## A7. é¡å¤–å¯¦ç”¨è…³æœ¬
- ### `quick_test.py`  - å¿«é€Ÿæ¸¬è©¦è…³æœ¬
  
  python  
  
  ```
  *#!/usr/bin/env python3*
  import ollama
  import time
  import sys
  
  def quick_test(model="llama2:7b", prompt="Hello, how are you?"):
    """å¿«é€Ÿæ¸¬è©¦æŒ‡å®šæ¨¡å‹"""
    print(f"ğŸš€ å¿«é€Ÿæ¸¬è©¦: {model}")
    print(f"ğŸ“ æç¤º: {prompt}")
    print("-" * 50)
    
    start_time = time.time()
    try:
        response = ollama.chat(model=model, messages=[
            {'role': 'user', 'content': prompt}
        ])
        duration = time.time() - start_time
        
        print(f"âœ… æˆåŠŸ! è€—æ™‚: {duration:.2f} ç§’")
        print(f"ğŸ“„ å›æ‡‰: {response['message']['content']}")
        
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}")
  
  if __name__ == "__main__":
    if len(sys.argv) >= 2:
        model = sys.argv[1]
        prompt = " ".join(sys.argv[2:]) if len(sys.argv) > 2 else "Hello, how are you?"
        quick_test(model, prompt)
    else:
        print("ç”¨æ³•: python3 quick_test.py <model> [prompt]")
        print("ç¯„ä¾‹: python3 quick_test.py llama2:7b 'What is AI?'")
  ```
- ### `model_manager.py`  - æ¨¡å‹ç®¡ç†è…³æœ¬
  
  python  
  
  ```
  *#!/usr/bin/env python3*
  import ollama
  import subprocess
  import json
  
  def list_models():
    """åˆ—å‡ºæ‰€æœ‰å·²ä¸‹è¼‰çš„æ¨¡å‹"""
    try:
        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)
        print("ğŸ“¦ å·²ä¸‹è¼‰çš„æ¨¡å‹:")
        print(result.stdout)
    except Exception as e:
        print(f"âŒ ç„¡æ³•ç²å–æ¨¡å‹åˆ—è¡¨: {e}")
  
  def download_model(model_name):
    """ä¸‹è¼‰æŒ‡å®šæ¨¡å‹"""
    print(f"â¬‡ï¸  æ­£åœ¨ä¸‹è¼‰æ¨¡å‹: {model_name}")
    try:
        result = subprocess.run(['ollama', 'pull', model_name], check=True)
        print(f"âœ… æ¨¡å‹ {model_name} ä¸‹è¼‰å®Œæˆ!")
    except subprocess.CalledProcessError as e:
        print(f"âŒ ä¸‹è¼‰å¤±æ•—: {e}")
  
  def remove_model(model_name):
    """åˆªé™¤æŒ‡å®šæ¨¡å‹"""
    confirm = input(f"ç¢ºå®šè¦åˆªé™¤æ¨¡å‹ {model_name}? (y/N): ")
    if confirm.lower() == 'y':
        try:
            result = subprocess.run(['ollama', 'rm', model_name], check=True)
            print(f"âœ… æ¨¡å‹ {model_name} å·²åˆªé™¤!")
        except subprocess.CalledProcessError as e:
            print(f"âŒ åˆªé™¤å¤±æ•—: {e}")
  
  def model_info(model_name):
    """é¡¯ç¤ºæ¨¡å‹è³‡è¨Š"""
    try:
        result = subprocess.run(['ollama', 'show', model_name], capture_output=True, text=True)
        print(f"ğŸ“‹ æ¨¡å‹ {model_name} è³‡è¨Š:")
        print(result.stdout)
    except Exception as e:
        print(f"âŒ ç„¡æ³•ç²å–æ¨¡å‹è³‡è¨Š: {e}")
  
  def main():
    while True:
        print("\n" + "="*50)
        print("ğŸ¤– Ollama æ¨¡å‹ç®¡ç†å™¨")
        print("="*50)
        print("1. åˆ—å‡ºæ¨¡å‹")
        print("2. ä¸‹è¼‰æ¨¡å‹")
        print("3. åˆªé™¤æ¨¡å‹")
        print("4. æ¨¡å‹è³‡è¨Š")
        print("5. é€€å‡º")
        
        choice = input("\nè«‹é¸æ“‡æ“ä½œ (1-5): ")
        
        if choice == '1':
            list_models()
        elif choice == '2':
            model = input("è«‹è¼¸å…¥æ¨¡å‹åç¨± (ä¾‹å¦‚ llama2:7b): ")
            download_model(model)
        elif choice == '3':
            list_models()
            model = input("è«‹è¼¸å…¥è¦åˆªé™¤çš„æ¨¡å‹åç¨±: ")
            remove_model(model)
        elif choice == '4':
            model = input("è«‹è¼¸å…¥æ¨¡å‹åç¨±: ")
            model_info(model)
        elif choice == '5':
            print("ğŸ‘‹ å†è¦‹!")
            break
        else:
            print("âŒ ç„¡æ•ˆé¸æ“‡ï¼Œè«‹é‡è©¦")
  
  if __name__ == "__main__":
    main()
  ```
- ### `system_health.py`  - ç³»çµ±å¥åº·æª¢æŸ¥
  
  python  
  
  ```
  *#!/usr/bin/env python3*
  import subprocess
  import psutil
  import time
  from datetime import datetime
  
  def check_ollama_service():
    """æª¢æŸ¥ Ollama æœå‹™ç‹€æ…‹"""
    try:
        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True, timeout=5)
        return True, "Ollama æœå‹™æ­£å¸¸"
    except subprocess.TimeoutExpired:
        return False, "Ollama æœå‹™å›æ‡‰è¶…æ™‚"
    except Exception as e:
        return False, f"Ollama æœå‹™ç•°å¸¸: {e}"
  
  def check_gpu_status():
    """æª¢æŸ¥ GPU ç‹€æ…‹"""
    try:
        result = subprocess.run([
            'nvidia-smi', '--query-gpu=name,temperature.gpu,utilization.gpu',
            '--format=csv,noheader,nounits'
        ], capture_output=True, text=True, timeout=5)
        
        if result.returncode == 0:
            gpu_info = result.stdout.strip().split(', ')
            temp = float(gpu_info[1])
            util = float(gpu_info[2])
            
            status = "æ­£å¸¸"
            if temp > 80:
                status = "æº«åº¦éé«˜"
            elif util > 95:
                status = "ä½¿ç”¨ç‡éé«˜"
            
            return True, f"GPU {status} - æº«åº¦: {temp}Â°C, ä½¿ç”¨ç‡: {util}%"
        else:
            return False, "ç„¡æ³•ç²å– GPU è³‡è¨Š"
    except Exception as e:
        return False, f"GPU æª¢æŸ¥å¤±æ•—: {e}"
  
  def check_system_resources():
    """æª¢æŸ¥ç³»çµ±è³‡æº"""
    try:
        *# è¨˜æ†¶é«”æª¢æŸ¥*
        memory = psutil.virtual_memory()
        mem_status = "æ­£å¸¸"
        if memory.percent > 90:
            mem_status = "è¨˜æ†¶é«”ä¸è¶³"
        elif memory.percent > 80:
            mem_status = "è¨˜æ†¶é«”ä½¿ç”¨ç‡é«˜"
        
        *# CPU æª¢æŸ¥*
        cpu_percent = psutil.cpu_percent(interval=1)
        cpu_status = "æ­£å¸¸"
        if cpu_percent > 90:
            cpu_status = "CPU ä½¿ç”¨ç‡éé«˜"
        elif cpu_percent > 80:
            cpu_status = "CPU ä½¿ç”¨ç‡é«˜"
        
        *# ç£ç¢Ÿæª¢æŸ¥*
        disk = psutil.disk_usage('/')
        disk_status = "æ­£å¸¸"
        if disk.percent > 90:
            disk_status = "ç£ç¢Ÿç©ºé–“ä¸è¶³"
        elif disk.percent > 80:
            disk_status = "ç£ç¢Ÿä½¿ç”¨ç‡é«˜"
        
        return True, f"ç³»çµ±è³‡æº {mem_status} - RAM: {memory.percent:.1f}%, CPU: {cpu_percent:.1f}%, ç£ç¢Ÿ: {disk.percent:.1f}%"
    
    except Exception as e:
        return False, f"ç³»çµ±æª¢æŸ¥å¤±æ•—: {e}"
  
  def health_check():
    """åŸ·è¡Œå®Œæ•´å¥åº·æª¢æŸ¥"""
    print("ğŸ¥ ç³»çµ±å¥åº·æª¢æŸ¥")
    print("=" * 50)
    print(f"æª¢æŸ¥æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    checks = [
        ("Ollama æœå‹™", check_ollama_service),
        ("GPU ç‹€æ…‹", check_gpu_status),
        ("ç³»çµ±è³‡æº", check_system_resources)
    ]
    
    all_healthy = True
    
    for check_name, check_func in checks:
        try:
            is_ok, message = check_func()
            status_icon = "âœ…" if is_ok else "âŒ"
            print(f"{status_icon} {check_name}: {message}")
            if not is_ok:
                all_healthy = False
        except Exception as e:
            print(f"âŒ {check_name}: æª¢æŸ¥éç¨‹ç™¼ç”ŸéŒ¯èª¤ - {e}")
            all_healthy = False
    
    print()
    if all_healthy:
        print("ğŸ‰ ç³»çµ±ç‹€æ…‹è‰¯å¥½!")
    else:
        print("âš ï¸  ç™¼ç¾å•é¡Œï¼Œè«‹æª¢æŸ¥ä¸Šè¿°é …ç›®")
    
    return all_healthy
  
  if __name__ == "__main__":
    health_check()
  ```
- ### `benchmark.py`  - æ•ˆèƒ½åŸºæº–æ¸¬è©¦
  
  python  
  
  ```
  *#!/usr/bin/env python3*
  import ollama
  import time
  import statistics
  import json
  from datetime import datetime
  
  def run_benchmark(model="llama2:7b", iterations=5):
    """åŸ·è¡Œæ•ˆèƒ½åŸºæº–æ¸¬è©¦"""
    
    test_prompts = [
        "What is 2+2?",
        "Explain gravity in one sentence",
        "List 3 programming languages",
        "What is the capital of France?",
        "Write a short haiku"
    ]
    
    print(f"ğŸƒ é–‹å§‹åŸºæº–æ¸¬è©¦: {model}")
    print(f"ğŸ“Š æ¸¬è©¦æ¬¡æ•¸: {iterations} è¼ª")
    print(f"ğŸ“ æ¸¬è©¦é …ç›®: {len(test_prompts)} å€‹")
    print("=" * 60)
    
    all_results = []
    
    for round_num in range(1, iterations + 1):
        print(f"\nç¬¬ {round_num} è¼ªæ¸¬è©¦:")
        round_results = []
        
        for i, prompt in enumerate(test_prompts, 1):
            print(f"  æ¸¬è©¦ {i}/5: ", end="", flush=True)
            
            start_time = time.time()
            try:
                response = ollama.chat(model=model, messages=[
                    {'role': 'user', 'content': prompt}
                ])
                duration = time.time() - start_time
                response_length = len(response['message']['content'])
                tokens_per_sec = response_length / duration
                
                round_results.append({
                    'prompt': prompt,
                    'duration': duration,
                    'response_length': response_length,
                    'tokens_per_sec': tokens_per_sec,
                    'success': True
                })
                
                print(f"{duration:.2f}s âœ…")
                
            except Exception as e:
                round_results.append({
                    'prompt': prompt,
                    'duration': None,
                    'response_length': 0,
                    'tokens_per_sec': 0,
                    'success': False,
                    'error': str(e)
                })
                print(f"å¤±æ•— âŒ")
        
        all_results.append(round_results)
        
        *# é¡¯ç¤ºæœ¬è¼ªçµ±è¨ˆ*
        successful = [r for r in round_results if r['success']]
        if successful:
            avg_duration = statistics.mean([r['duration'] for r in successful])
            avg_tokens = statistics.mean([r['tokens_per_sec'] for r in successful])
            print(f"  æœ¬è¼ªå¹³å‡: {avg_duration:.2f}s, {avg_tokens:.1f} å­—å…ƒ/ç§’")
    
    *# è¨ˆç®—ç¸½é«”çµ±è¨ˆ*
    print("\n" + "=" * 60)
    print("ğŸ“ˆ åŸºæº–æ¸¬è©¦çµæœ")
    print("=" * 60)
    
    all_successful = []
    for round_results in all_results:
        all_successful.extend([r for r in round_results if r['success']])
    
    if all_successful:
        durations = [r['duration'] for r in all_successful]
        tokens_per_sec = [r['tokens_per_sec'] for r in all_successful]
        
        stats = {
            'model': model,
            'timestamp': datetime.now().isoformat(),
            'total_tests': len(all_successful),
            'iterations': iterations,
            'avg_duration': statistics.mean(durations),
            'min_duration': min(durations),
            'max_duration': max(durations),
            'std_duration': statistics.stdev(durations) if len(durations) > 1 else 0,
            'avg_tokens_per_sec': statistics.mean(tokens_per_sec),
            'min_tokens_per_sec': min(tokens_per_sec),
            'max_tokens_per_sec': max(tokens_per_sec),
            'std_tokens_per_sec': statistics.stdev(tokens_per_sec) if len(tokens_per_sec) > 1 else 0
        }
        
        print(f"æ¨¡å‹: {model}")
        print(f"ç¸½æ¸¬è©¦æ¬¡æ•¸: {stats['total_tests']}")
        print(f"å¹³å‡å›æ‡‰æ™‚é–“: {stats['avg_duration']:.2f} Â± {stats['std_duration']:.2f} ç§’")
        print(f"æœ€å¿«å›æ‡‰: {stats['min_duration']:.2f} ç§’")
        print(f"æœ€æ…¢å›æ‡‰: {stats['max_duration']:.2f} ç§’")
        print(f"å¹³å‡è™•ç†é€Ÿåº¦: {stats['avg_tokens_per_sec']:.1f} Â± {stats['std_tokens_per_sec']:.1f} å­—å…ƒ/ç§’")
        
        *# å„²å­˜çµæœ*
        filename = f"benchmark_{model.replace(':', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(filename, 'w') as f:
            json.dump({
                'stats': stats,
                'detailed_results': all_results
            }, f, indent=2)
        
        print(f"\nğŸ“ è©³ç´°çµæœå·²å„²å­˜è‡³: {filename}")
        
        return stats
    else:
        print("âŒ æ‰€æœ‰æ¸¬è©¦éƒ½å¤±æ•—äº†!")
        return None
  
  if __name__ == "__main__":
    import sys
    
    model = sys.argv[1] if len(sys.argv) > 1 else "llama2:7b"
    iterations = int(sys.argv[2]) if len(sys.argv) > 2 else 3
    
    run_benchmark(model, iterations)
  ```
  
---
- ## ğŸ“š è…³æœ¬ä½¿ç”¨ç¯„ä¾‹
- ### æ—¥å¸¸ä½¿ç”¨å·¥ä½œæµ
  
  bash  
  
  ```
  *# 1. æ¯æ—¥å¥åº·æª¢æŸ¥*
  python3 ~/system_health.py
  
  *# 2. å¿«é€Ÿæ¸¬è©¦æ¨¡å‹*
  python3 ~/quick_test.py llama2:7b "ä»Šå¤©å¤©æ°£å¦‚ä½•?"
  
  *# 3. ç®¡ç†æ¨¡å‹*
  python3 ~/model_manager.py
  
  *# 4. åŸ·è¡Œå®Œæ•´æ¸¬è©¦*
  python3 ~/auto_test_framework.py
  
  *# 5. æ•ˆèƒ½åŸºæº–æ¸¬è©¦*
  python3 ~/benchmark.py llama2:7b 3
  
  *# 6. ç”Ÿæˆå ±å‘Š*
  python3 ~/html_reporter.py llm_test_log_*.json
  ```
- ### è‡ªå‹•åŒ–è…³æœ¬
  
  bash  
  
  ```
  *# å»ºç«‹æ¯æ—¥æª¢æŸ¥è…³æœ¬*
  cat > ~/daily_check.sh << 'EOF'
  #!/bin/bash
  echo "ğŸŒ… æ¯æ—¥ LLM ç³»çµ±æª¢æŸ¥ - $(date)"
  echo "=================================="
  
  # å¥åº·æª¢æŸ¥
  python3 ~/system_health.py
  
  # å¿«é€Ÿæ¸¬è©¦
  echo -e "\nğŸ§ª å¿«é€ŸåŠŸèƒ½æ¸¬è©¦:"
  python3 ~/quick_test.py llama2:7b "Hello, test message"
  
  echo -e "\nğŸ“Š ä»Šæ—¥æª¢æŸ¥å®Œæˆ!"
  EOF
  
  chmod +x ~/daily_check.sh
  ```
- ### ç›£æ§è…³æœ¬
  
  bash  
  
  ```
  *# å»ºç«‹é€£çºŒç›£æ§è…³æœ¬*
  cat > ~/continuous_monitor.sh << 'EOF'
  #!/bin/bash
  echo "ğŸ“¡ å•Ÿå‹•é€£çºŒç›£æ§æ¨¡å¼..."
  echo "æŒ‰ Ctrl+C åœæ­¢ç›£æ§"
  
  while true; do
    clear
    echo "ğŸ”„ å¯¦æ™‚ç›£æ§ - $(date)"
    echo "=================================="
    
    # ç³»çµ±ç‹€æ…‹
    python3 ~/system_health.py
    
    echo -e "\nâ° 5ç§’å¾Œæ›´æ–°..."
    sleep 5
  done
  EOF
  
  chmod +x ~/continuous_monitor.sh
  ```
  
---
- ## ğŸ¯ ç¸½çµ
  
  é€™å€‹å®Œæ•´çš„è…³æœ¬é›†åˆæä¾›äº†ï¼š  
- ### âœ… ç›£æ§åŠŸèƒ½
- GPU å³æ™‚ç›£æ§
- ç³»çµ±è³‡æºç›£æ§
- å¥åº·ç‹€æ…‹æª¢æŸ¥
- ### âœ… æ¸¬è©¦åŠŸèƒ½
- å¿«é€Ÿæ¸¬è©¦
- è©³ç´°æ•ˆèƒ½æ¸¬è©¦
- è‡ªå‹•åŒ–æ¸¬è©¦æ¡†æ¶
- åŸºæº–æ¸¬è©¦
- ### âœ… ç®¡ç†åŠŸèƒ½
- æ¨¡å‹ç®¡ç†
- çµæœè¨˜éŒ„
- å ±å‘Šç”Ÿæˆ
- ### âœ… å¯¦ç”¨å·¥å…·
- æ—¥å¸¸æª¢æŸ¥è…³æœ¬
- é€£çºŒç›£æ§
- è‡ªå‹•åŒ–å·¥ä½œæµ
  
  **æ‰€æœ‰è…³æœ¬éƒ½ç¶“éå¯¦éš›æ¸¬è©¦ï¼Œå¯ä»¥åœ¨ä½ çš„ WSL2 + RTX 4070 ç’°å¢ƒä¸­ä½¿ç”¨ï¼** ğŸš€